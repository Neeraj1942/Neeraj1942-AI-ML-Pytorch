"Unsupervised learning models are trained without any target variable; they learn patterns or structures from the input data itself."
examples ->
1. identify the age group for the banking customer
2. Personalizing adds based on a website personalized
3. giving similar image suggestions to the customer

Our problem statement : To arrange pictures in a library according to the context/content in them
Introduction to ->
Autoencoders : input -> features -> image reconstructer -> input
| Component   | Purpose                                       | Input Shape                | Output Shape               | Notes                                          |
| ----------- | --------------------------------------------- | -------------------------- | -------------------------- | ---------------------------------------------- |
| **Encoder** | Compresses input to latent representation     | `(batch_size, input_dim)`  | `(batch_size, latent_dim)` | Learns key features; reduces dimensionality    |
| **Decoder** | Reconstructs input from latent representation | `(batch_size, latent_dim)` | `(batch_size, input_dim)`  | Tries to make output similar to original input |

| Component   | Input | Output | Purpose                        |
| ----------- | ----- | ------ | ------------------------------ |
| Encoder     | x     | z      | Compress input to latent space |
| Decoder     | z     | x̂     | Reconstruct input from latent  |
| Autoencoder | x     | x̂     | Minimize reconstruction loss   |

code : 
in the model we ->
# define architecture of autoencoder

## this is our input placeholder
input_img = Input(shape=(784, ))

## "encoded" is the encoded representation of the input
encoded = Dense(2000, activation='relu')(input_img)
encoded = Dense(500, activation='relu')(encoded)
encoded = Dense(100, activation='relu')(encoded)
encoded = Dense(10, activation='linear')(encoded)

## "decoded" is the lossy reconstruction of the input
decoded = Dense(100, activation='relu')(encoded)
decoded = Dense(500, activation='relu')(decoded)
decoded = Dense(784, activation='sigmoid')(decoded)

## this model maps an input to its reconstruction
autoencoder = Model(input_img, decoded)

## this model maps an input to its features
encoder = Model(input_img, encoded)


we use : autoencoder = Model(input_img, decoded)
Maps input → reconstructed output.
This is what we train using a loss like MSE or binary cross-entropy.
Goal: Make decoded as close as possible to input_img.

encoder = Model(input_img, encoded)
Maps input → compressed features (length 10).
This is used after training to get the features of input data.
You don’t train this separately; it shares the same weights as the encoder part of the autoencoder.

Note : 
Autoencoder: Maps Input → Encoded → Decoded.
Full model: autoencoder = Model(input_img, decoded)
Encoder: Maps only Input → Encoded.
Partial model: encoder = Model(input_img, encoded)
Reuses the same layers as the autoencoder up to the encoded layer.

# check output of autoencoder model
temp = autoencoder.predict(train_x)
plt.imshow(temp[0].reshape((28, 28)), cmap='gray')

plt.imshow(temp[0].reshape((28, 28)), cmap='gray')
-> we make the 784 values from 1-d to 2-d using reshape(28,28) 
temp = autoencoder.predict(train_x) -> 
temp[0] - gives 784 data points as the last decoder is 784 values

but when we do ->
# check output of encoder model
temp = encoder.predict(train_x)
temp[0]
we get 10 rows because the last encoder is set to 10 rows.



# get actual classes
train_y = train.label.values -> train.label accesses the column named "label" in the DataFrame
train_y, val_y = train_y[:split_size], train_y[split_size:]

# compare clusters with actual classes
temp = pd.DataFrame({"val_y":val_y, "cluster_name":pred})
temp[temp.cluster_name == 1].head()

train_y = train.label.values          # gets actual labels from the dataset
train_y, val_y = train_y[:split_size], train_y[split_size:]  # split into train/val

temp = pd.DataFrame({"val_y": val_y, "cluster_name": pred})  # create DataFrame
temp[temp.cluster_name == 1].head()  # filter rows where cluster == 1




# train a kmeans clustering model to categorize the features of images

## define kmeans model
km = KMeans(n_clusters=10) 
we use 10 cluster because the images are from 0---9 : 10


# visualize multiple images of cluster
## create plot
fig,axes = plt.subplots(nrows=5,ncols=5,figsize=(10,10))


## set row name on the basis of clusters
rows = ['Images belong to Cluster {}'.format(row) for row in ['1', '2', '3', '4', '5']]
for ax, row in zip(axes[:,0], rows):
    ax.annotate(row, xy=(0, 0.5), xytext=(-ax.yaxis.labelpad - 5, 0),
                xycoords=ax.yaxis.label, textcoords='offset points',
                size='large', ha='right', va='center')

## set images in subplots
for i in range(5):
  temp2 = temp[temp.cluster_name == i].head() 
  for j, img in enumerate(val_x[temp2.index.values]):
    img = img.reshape((28, 28))
    axes[i][j].imshow(img, cmap='gray')

## adjust plot with respect to the column title
fig.tight_layout()
fig.subplots_adjust(left=0.15, top=0.95)

## show plot
plt.show()

to print what the 10 clusters look like the images attached to each.


# print score of overall model
normalized_mutual_info_score(pred, val_y)
to get the scores of the similarity


list of most famously used alogorithms for unsupervised learning are : 
| Algorithm                                 | Why it’s considered “successful”                                                                                             | Short explanation                                                                                                                                               |
| ----------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| SimCLR                                    | One of the early breakthroughs in contrastive self‑supervision; simple yet strong. ([mmselfsup.readthedocs.io][1])           | Learns representations by contrasting augmented views of the same sample (“positive pair”) versus others (“negatives”). Good linear‑probe accuracy on ImageNet. |
| MoCo v2 (Momentum Contrast v2)            | Improves on SimCLR by needing smaller batches, more efficient memory queue. ([Emergent Mind][2])                             | Uses a momentum encoder + queue of negative samples to enable good unsupervised pre‑training with less extreme hardware/batch size.                             |
| BYOL (Bootstrap Your Own Latent)          | Shows strong performance without explicit negative pairs (in some settings). ([Maike Behrendt, PhD student][3])              | Learns by comparing two network outputs (online/target) of differently augmented views of same image; no need for explicit “negative” samples.                  |
| SwAV (Swapping Assignments between Views) | Combines clustering + contrastive/slightly different objective; strong on transfer tasks. ([Maike Behrendt, PhD student][3]) | Learns representations by assigning features of views to prototype clusters and swappings assignments across views.                                             |
| Barlow Twins                              | Recent method focusing on invariance and redundancy reduction; competitive SOTA. ([ResearchGate][4])                         | Enforces representation dimensions to be decorrelated (“redundancy reduction”) while keeping invariance across augmented views.                                 |
