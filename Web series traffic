Time Series :
Series of data points listed in time order

The time series data can be ->
Stationary
Seasonality
Autocorrelated

Forecasting web traffic.ipynb
Code :
1. Load the dataset
2. Data Exploration
To see the whole data at once ->
import matplotlib.pyplot as plt
sessions = data['Sessions'].values -> .values(): convert it into a numpy array

ar = np.arange(len(sessions)) -> .arrange(): creates numpy array from 0 to n-1 
plt.figure(figsize=(22,10))
plt.plot(ar, sessions,'r')
plt.show()

np.arange(start, stop, step) -> method helps to make an numpy array

#first week web traffic
sample = sessions[:168] -> 168/7 = 24 -> means 1 weeks hourly data
ar = np.arange(len(sample))
plt.figure(figsize=(22,10))
plt.plot(ar, sample,'r')
plt.show()

Data Preparation ->
def prepare_data(seq,num):
  x=[]
  y=[]

  for i in range(0,(len(seq)-num),1):
    
    input_ = seq[i:i+num]
    output  = seq[i+num]
    
    x.append(input_)
    y.append(output)
    
  return np.array(x), np.array(y)
example usage of the code :
| i | input_             | output |
| - | ------------------ | ------ |
| 0 | seq[0:2] = [10,20] | 30     |
| 1 | seq[1:3] = [20,30] | 40     |
| 2 | seq[2:4] = [30,40] | 50     |

then we call the actual data 'sessions' using the function :
num=168
x,y= prepare_data(sessions,num)
 now we will get ->
x: each item is a sequence of 168 past sessions
y: each item is the next session value after those 168
example of this is :
sessions = [10, 20, 30, 40, 50, 60]
num = 3

x = [
  [10, 20, 30],
  [20, 30, 40],
  [30, 40, 50]
]

y = [
  40,
  50,
  60
]

what happens here is - afteer we use the data preparation code , we use 
x as inputs values of a series of 168 values 
y then predicts the 169th value, of the 168 series.

Splitting the dataset into training and validation ->
here we do this manually using and integer, and manually slicing the numpy array
that is ->
ind = int(0.9 * len(x))

x_tr = x[:ind]
y_tr = y[:ind]
x_val=x[ind:]
y_val=y[ind:]

| Variable | Shape            | Meaning                                |
| -------- | ---------------- | -------------------------------------- |
| `x`      | `(samples, 168)` | Each row = one 168-step input sequence | 2-d 
| `y`      | `(samples,)`     | Each value = the next single value     | 1-d 

then we normalize the inputs/outputs ->
for train we use fit_transform() ; validation we do transform() 

as the y variable is 1-d and we have to reshape them to atleast 2-d to use the standard scaler to normalize them

y_tr=y_tr.reshape(len(y_tr),1)
y_val=y_val.reshape(len(y_val),1)

#normalize the output
y_scaler=StandardScaler()
y_tr = y_scaler.fit_transform(y_tr)[:,0]
y_val = y_scaler.transform(y_val)[:,0]

array[:, 0] -> makes sure to convert the 2-d to 1-d array. 
The : means “take all rows”.
The 0 means “take the first column” (columns are 0-indexed).

basically then we reshape it m=to make it ready for lastm and model building :
x_tr = x_tr.reshape(x_tr.shape[0], x_tr.shape[1], 1)
x_val = x_val.reshape(x_val.shape[0], x_val.shape[1], 1)

x_tr.shape[0] → number of samples (rows)
x_tr.shape[1] → number of timesteps (columns)
1 → number of features per timestep

so after reshaping : print(x_tr.shape) -> output : (4255, 168, 1)

model building ->
conv1d -> cnn layer for sequences

Comparision with the baseline model :
