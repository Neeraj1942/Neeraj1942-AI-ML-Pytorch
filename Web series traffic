Time Series :
Series of data points listed in time order

The time series data can be ->
Stationary
Seasonality
Autocorrelated

Forecasting web traffic.ipynb
Code :
1. Load the dataset
2. Data Exploration
To see the whole data at once ->
import matplotlib.pyplot as plt
sessions = data['Sessions'].values -> .values(): convert it into a numpy array

ar = np.arange(len(sessions)) -> .arrange(): creates numpy array from 0 to n-1 
plt.figure(figsize=(22,10))
plt.plot(ar, sessions,'r')
plt.show()

np.arange(start, stop, step) -> method helps to make an numpy array

#first week web traffic
sample = sessions[:168] -> 168/7 = 24 -> means 1 weeks hourly data
ar = np.arange(len(sample))
plt.figure(figsize=(22,10))
plt.plot(ar, sample,'r')
plt.show()

Data Preparation ->
def prepare_data(seq,num):
  x=[]
  y=[]

  for i in range(0,(len(seq)-num),1):
    
    input_ = seq[i:i+num]
    output  = seq[i+num]
    
    x.append(input_)
    y.append(output)
    
  return np.array(x), np.array(y)
example usage of the code :
| i | input_             | output |
| - | ------------------ | ------ |
| 0 | seq[0:2] = [10,20] | 30     |
| 1 | seq[1:3] = [20,30] | 40     |
| 2 | seq[2:4] = [30,40] | 50     |

then we call the actual data 'sessions' using the function :
num=168
x,y= prepare_data(sessions,num)
 now we will get ->
x: each item is a sequence of 168 past sessions
y: each item is the next session value after those 168
example of this is :
sessions = [10, 20, 30, 40, 50, 60]
num = 3

x = [
  [10, 20, 30],
  [20, 30, 40],
  [30, 40, 50]
]

y = [
  40,
  50,
  60
]

what happens here is - afteer we use the data preparation code , we use 
x as inputs values of a series of 168 values 
y then predicts the 169th value, of the 168 series.

Splitting the dataset into training and validation ->
here we do this manually using and integer, and manually slicing the numpy array
that is ->
ind = int(0.9 * len(x))

x_tr = x[:ind]
y_tr = y[:ind]
x_val=x[ind:]
y_val=y[ind:]

| Variable | Shape            | Meaning                                |
| -------- | ---------------- | -------------------------------------- |
| `x`      | `(samples, 168)` | Each row = one 168-step input sequence | 2-d 
| `y`      | `(samples,)`     | Each value = the next single value     | 1-d 

then we normalize the inputs/outputs ->
for train we use fit_transform() ; validation we do transform() 

as the y variable is 1-d and we have to reshape them to atleast 2-d to use the standard scaler to normalize them

y_tr=y_tr.reshape(len(y_tr),1)
y_val=y_val.reshape(len(y_val),1)

#normalize the output
y_scaler=StandardScaler()
y_tr = y_scaler.fit_transform(y_tr)[:,0]
y_val = y_scaler.transform(y_val)[:,0]

array[:, 0] -> makes sure to convert the 2-d to 1-d array. 
The : means “take all rows”.
The 0 means “take the first column” (columns are 0-indexed).

basically then we reshape it m=to make it ready for lastm and model building :
x_tr = x_tr.reshape(x_tr.shape[0], x_tr.shape[1], 1)
x_val = x_val.reshape(x_val.shape[0], x_val.shape[1], 1)

x_tr.shape[0] → number of samples (rows)
x_tr.shape[1] → number of timesteps (columns)
1 → number of features per timestep

so after reshaping : print(x_tr.shape) -> output : (4255, 168, 1)

model building ->
conv1d -> cnn layer for sequences

Comparision with the baseline model : we use moving averages ->
#build a simple model
def compute_moving_average(data):
  pred=[]
  for i in data:
    avg=np.sum(i)/len(i)
    pred.append(avg)
  return np.array(pred)

just the average of the each list i of 168 elements in the whole array, and calc the avg

Note : Helps you detect overfitting or underfitting
If your neural network has very low training error but the baseline is already good, 
and validation error is high → likely overfitting.
Baseline shows how “hard” the problem is. Some time series are smooth; 
sometimes, even a moving average does surprisingly well.

So if temp originally had shape (10,) (1D array of 10 elements), 
after temp.reshape(1, -1, 1) : The shape becomes (1, 10, 1)
1 “batch”, 10 “timesteps”, 1 “feature” — 
often used in machine learning for models that expect 3D input like LSTMs.

def forecast(x_val, no_of_pred, ind):
  predictions=[]

  #intialize the array with previous weeks data  
  temp=x_val[ind]

  for i in range(no_of_pred): 

    #predict for the next hour
    pred=model.predict(temp.reshape(1,-1,1))[0][0]
    
    #append the prediction as the last element of array
    temp = np.insert(temp,len(temp),pred)
    predictions.append(pred)

    #ignore the first element of array
    temp = temp[1:]

  return predictions

example with values : 
num = 3 (to simplify),
x_val[ind] = [10, 12, 14] (last 3 hours of traffic),
no_of_pred = 2 (we want next 2 hours).

Iteration 1
Input to model: [10, 12, 14]
Model predicts: 16
Append prediction: temp = [10, 12, 14, 16]
Remove first element: temp = [12, 14, 16]
predictions = [16]

Iteration 2
Input to model: [12, 14, 16]
Model predicts: 18
Append prediction: temp = [12, 14, 16, 18]
Remove first element: temp = [14, 16, 18]
predictions = [16, 18]
✅ Result: forecast(x_val, 2, ind) returns [16, 18]

temp = np.insert(temp,len(temp),pred) ->
temp is the current sequence, pred is the next predicted value;
np.insert(temp, len(temp), pred) appends pred to the end of temp for the next forecast step.

Example : 
y_val = [10, 20, 30, 40, 50, 60]
ind = 2
no_of_pred = 3

y_true = y_val[ind:ind+no_of_pred]
print(y_true)  # Output: [30, 40, 50]

