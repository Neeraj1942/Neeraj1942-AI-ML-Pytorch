Understanding the working of Neural Networks
Deep Learning ->

Machine Learning ->
1. Learning from the data
2. Making predictions

Good for use case->
1. Structured data and simpler tasks
2. Requires less data to learn
3. cpus smaller servor
4. less time to train

Deep Learning ->
1. Algorithm Approach
2. Data Requirement
3. Types of Problems

Good for use case->
1. Unstrcuted data and complex tasks
2. Requires more/large data to learn
3. gpus and cpus required, tpus
4. 

Computer Vision -> field of study which deals with extracting inofrmation from images and videos
NLP -> Field of study which extracts information from text

Applications of deep learning ->
Language interpretations - > Google assistance, chatgpt
Recommendation systems -> spotify, netflix
Image recognition and computer vision -> face id , tesla autopilot

Terminology ->
Nodes : Basic processing unit of any neural network
Layer : Is an group of nodes performing the same task.
Input layer, Hidden layer, Output layer

-> Binary classification/ Regression has only 1 output node. 
-> Multi classification has as many as the class has in the output node.
Depth of a neural network : number of hidden layers

forward propogation : weights * input + bias at each step and acrtivation function , so on till the output. 
backward progration : error is added and feed-back to the previous layers then we use that for backward propogation, till the input.
                    : so they also help in uodating the weights accordingly to get the best accuracy.

1 cycle : 1 forward propogation and 1 backward progration makes 1 cycle.
Now the NN is ready for actual prediction.

Deep learning frameworks ->
1. Designed for scalability : Managing large datasets with ease
2. Updates : Ensuring users stay up-to-date with the latest features.
Examples ->
1. TensorFlow - Ml and Dl models. , 2. Pytorch - Computer vision and NLP.

Using Pytorch ->

Importing libraries ->
import torch
import torch.nn as nn

inputs to neurons ->
inputs = torch.tensor([3.0, 2.0, -1.0])

Creating a neuron ->
neuron = nn.Linear(in_features = 3, out_features = 1)
output = neurons(inputs)

Printing the output neurons ->
print(output) 

neuron weight ->
neuron.weight

neuron bias ->
neuron.bias

Formula for the final output of the neuron ->
neuron.weight@inputs.T + neurons.bias       (.T stands for transpose -> 1*3 .T goes to 3*1 , weights - 1*3 , output - 1 )

We can eityher use matmul() -> torch.matmul() or '@' both are used for matriz multiplication
Using matmul -> torch.matmul(neuron.weight, input) + neuron.bias 

Activation Function for the last layer ->
Regression :
1. Linear  -> with negative values
2. RelU    -> only postive values

Classification :
3. Sigmoid -> Binary  (0 to 1)
4. SoftMax -> Multi  Classification -> all positive, marks them to 1

Loss Function ->
Also known as cost function : measures the network's error prediction 

Types ->
Classification ->
1. Binary cross entropy/ Log Loss : 
Loss=−[y⋅log(p)+(1−y)⋅log(1−p)] -> postive : -log(p) ; negaitve is -log(1-p)
Lower log loss = better performance 

2. Category cross entropy loss : -summation(yi*log(p))
when we consider a example of vehicle type : we coonvert it to one hot encoding, later the probaliity of others is 0 and the predicted vehicle is 1.

Regression ->
1. Mean Squared Error
if outliers exists -> then we use MAE( Mean absolute error) , Ex : realtime time for delivery.

Example -> Predict the gpa of students, from their high school gpa's and sat scores.
we have to convert our data into numpy format because it can later be converted to tensor and used for deep learning.

 X = data[['a', 'b']].values
output :
[[1 4]
 [2 5]
 [3 6]]

X = data['c'].values 
output :
array([7, 8, 9, 10, 11])  # shape: (5,)

X = data['c'].values.reshape(-1, 1)
output :
array([[7],
       [8],
       [9],
       [10],
       [11]])  # shape: (5, 1)

Steps ->
Load Dataset
Data Preprocessing 
Convert the data into tensors

Bulding the model ->
import torch.nn as nn

Building model with 2 neurons ->
model = nn.Sequential(
nn.Linear(2,2)       ->  # input layer: 2 features to 2 neurons
nn.Sigmoid(),        ->  # activation function
nn.Linear(2,1)       ->  # output layer: 2 neurons to 1 output 
)
Note : output of hidden layer should match the input size of the output layer

Forward propogation ->
preds = model(X_train_tensor)
preds[:5]

from torch.nn import MSEloss 

Calculating loss ->
criterion = MSELoss()
loss = criterion(preds,y_train_tensor)
print(loss)

Note : Loss of 0-5 is good for MSE, more than that is very high

Comparing predictions on X_train with target(y_train_tensor) ->
preds[:5]
y_train_tensor[:5]
model[0].weight -> input to hidden ( 0:Linear) 
model[2].weight -> hidden to output( 2: Linear)
model[1] ->  activation function , no weights attached instead it uses mathematical conversion to the input data to introduce non-linearlity.


Optimizing the errors -> (Minimizing the loss)
Gradient Descent ->
When plotted a gragh - loss function ( y-axis) and weights (x -axis)
New weight = old weight – (learning rate) × slope (gradient)
this helps us to converge to the point where the error is zero

Stop when ->
1. new weight = old weight ( i.e slope is 0)
2. Number of iteration =1000 ( then we can check the closest point with less error and use that)
Note : normally learning rate = 0.001

New weight = old weight – (learning rate) × slope (gradient)
New bias = old bias – (learning rate) × slope (gradient)

dl/dw = 2*u(mean)*xi = 2((w*xi +b) -y*xi)
dl/dB = 2u = 2(w(old) * X + b) - y

For multiple layes (i.e 2 hidden layers)->
| **Weights: Input Layer**                                                     | **Weights: From Hidden Layer**                                               | **Bias Terms**                                                          |
|-----------------------------------------------------------------------------|-----------------------------------------------------------------------------|-------------------------------------------------------------------------|
| \( w_{11}^1(\text{new}) = w_{11}^1 - \eta \frac{\partial L}{\partial w_{11}^1} \) | \( w_{11}^2(\text{new}) = w_{11}^2 - \eta \frac{\partial L}{\partial w_{11}^2} \) | \( b_{11}(\text{new}) = b_{11} - \eta \frac{\partial L}{\partial b_{11}} \) |
| \( w_{12}^1(\text{new}) = w_{12}^1 - \eta \frac{\partial L}{\partial w_{12}^1} \) | \( w_{21}^2(\text{new}) = w_{21}^2 - \eta \frac{\partial L}{\partial w_{21}^2} \) | \( b_{12}(\text{new}) = b_{12} - \eta \frac{\partial L}{\partial b_{12}} \) |
| \( w_{21}^1(\text{new}) = w_{21}^1 - \eta \frac{\partial L}{\partial w_{21}^1} \) |                                                                             | \( b_{21}(\text{new}) = b_{21} - \eta \frac{\partial L}{\partial b_{21}} \) |
| \( w_{22}^1(\text{new}) = w_{22}^1 - \eta \frac{\partial L}{\partial w_{22}^1} \) |                                                                             |                                                                         |





