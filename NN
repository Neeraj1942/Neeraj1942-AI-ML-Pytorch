Understanding the working of Neural Networks
Deep Learning ->

Machine Learning ->
1. Learning from the data
2. Making predictions

Good for use case->
1. Structured data and simpler tasks
2. Requires less data to learn
3. cpus smaller servor
4. less time to train

Deep Learning ->
1. Algorithm Approach
2. Data Requirement
3. Types of Problems

Good for use case->
1. Unstrcuted data and complex tasks
2. Requires more/large data to learn
3. gpus and cpus required, tpus
4. 

Computer Vision -> field of study which deals with extracting inofrmation from images and videos
NLP -> Field of study which extracts information from text

Applications of deep learning ->
Language interpretations - > Google assistance, chatgpt
Recommendation systems -> spotify, netflix
Image recognition and computer vision -> face id , tesla autopilot

Terminology ->
Nodes : Basic processing unit of any neural network
Layer : Is an group of nodes performing the same task.
Input layer, Hidden layer, Output layer

-> Binary classification/ Regression has only 1 output node. 
-> Multi classification has as many as the class has in the output node.
Depth of a neural network : number of hidden layers

forward propogation : weights * input + bias at each step and acrtivation function , so on till the output. 
backward progration : error is added and feed-back to the previous layers then we use that for backward propogation, till the input.
                    : so they also help in uodating the weights accordingly to get the best accuracy.

1 cycle : 1 forward propogation and 1 backward progration makes 1 cycle.
Now the NN is ready for actual prediction.

Deep learning frameworks ->
1. Designed for scalability : Managing large datasets with ease
2. Updates : Ensuring users stay up-to-date with the latest features.
Examples ->
1. TensorFlow - Ml and Dl models. , 2. Pytorch - Computer vision and NLP.

Using Pytorch ->

Importing libraries ->
import torch
import torch.nn as nn

inputs to neurons ->
inputs = torch.tensor([3.0, 2.0, -1.0])

Creating a neuron ->
neuron = nn.Linear(in_features = 3, out_features = 1)
output = neurons(inputs)

Printing the output neurons ->
print(output) 

neuron weight ->
neuron.weight

neuron bias ->
neuron.bias

Formula for the final output of the neuron ->
neuron.weight@inputs.T + neurons.bias       (.T stands for transpose -> 1*3 .T goes to 3*1 , weights - 1*3 , output - 1 )

We can eityher use matmul() -> torch.matmul() or '@' both are used for matriz multiplication
Using matmul -> torch.matmul(neuron.weight, input) + neuron.bias 

Activation Function for the last layer ->
Regression :
1. Linear  -> with negative values
2. RelU    -> only postive values

Classification :
3. Sigmoid -> Binary  (0 to 1)
4. SoftMax -> Multi  Classification -> all positive, marks them to 1

Loss Function ->
Also known as cost function : measures the network's error prediction 

Types ->
Classification ->
1. Binary cross entropy/ Log Loss : 
Loss=−[y⋅log(p)+(1−y)⋅log(1−p)] -> postive : -log(p) ; negaitve is -log(1-p)
Lower log loss = better performance 

2. Category cross entropy loss : -summation(yi*log(p))
when we consider a example of vehicle type : we coonvert it to one hot encoding, later the probaliity of others is 0 and the predicted vehicle is 1.

Regression ->
1. Mean Squared Error
if outliers exists -> then we use MAE( Mean absolute error) , Ex : realtime time for delivery.

Example -> Predict the gpa of students, from their high school gpa's and sat scores.
we have to convert our data into numpy format because it can later be converted to tensor and used for deep learning.

 X = data[['a', 'b']].values
output :
[[1 4]
 [2 5]
 [3 6]]

X = data['c'].values 
output :
array([7, 8, 9, 10, 11])  # shape: (5,)

X = data['c'].values.reshape(-1, 1)
output :
array([[7],
       [8],
       [9],
       [10],
       [11]])  # shape: (5, 1)

Steps ->
Load Dataset
Data Preprocessing 
Convert the data into tensors

Bulding the model ->
import torch.nn as nn

Building model with 2 neurons ->
model = nn.Sequential(
nn.Linear(2,2)       ->  # input layer: 2 features to 2 neurons
nn.Sigmoid(),        ->  # activation function
nn.Linear(2,1)       ->  # output layer: 2 neurons to 1 output 
)
Note : output of hidden layer should match the input size of the output layer

Forward propogation ->
preds = model(X_train_tensor)
preds[:5]

from torch.nn import MSEloss 

Calculating loss ->
criterion = MSELoss()
loss = criterion(preds,y_train_tensor)
print(loss)

Note : Loss of 0-5 is good for MSE, more than that is very high

Comparing predictions on X_train with target(y_train_tensor) ->
preds[:5]
y_train_tensor[:5]
model[0].weight -> input to hidden ( 0:Linear) 
model[2].weight -> hidden to output( 2: Linear)
model[1] ->  activation function , no weights attached instead it uses mathematical conversion to the input data to introduce non-linearlity.


Optimizing the errors -> (Minimizing the loss)
Gradient Descent ->
When plotted a gragh - loss function ( y-axis) and weights (x -axis)
New weight = old weight – (learning rate) × slope (gradient)
this helps us to converge to the point where the error is zero

Stop when ->
1. new weight = old weight ( i.e slope is 0)
2. Number of iteration =1000 ( then we can check the closest point with less error and use that)
Note : normally learning rate = 0.001

New weight = old weight – (learning rate) × slope (gradient)
New bias = old bias – (learning rate) × slope (gradient)

dl/dw = 2*u(mean)*xi = 2((w*xi +b) -y*xi)
dl/dB = 2u = 2(w(old) * X + b) - y

For multiple layes (i.e 2 hidden layers)->
| **Weights: Input Layer**                                                      | **Weights: From Hidden Layer**                                                | **Bias Terms**                                                          |
| ----------------------------------------------------------------------------- | ----------------------------------------------------------------------------- | ----------------------------------------------------------------------- |
| $w_{11}^1(\text{new}) = w_{11}^1 - \eta \frac{\partial L}{\partial w_{11}^1}$ | $w_{11}^2(\text{new}) = w_{11}^2 - \eta \frac{\partial L}{\partial w_{11}^2}$ | $b_{11}(\text{new}) = b_{11} - \eta \frac{\partial L}{\partial b_{11}}$ |
| $w_{12}^1(\text{new}) = w_{12}^1 - \eta \frac{\partial L}{\partial w_{12}^1}$ | $w_{21}^2(\text{new}) = w_{21}^2 - \eta \frac{\partial L}{\partial w_{21}^2}$ | $b_{12}(\text{new}) = b_{12} - \eta \frac{\partial L}{\partial b_{12}}$ |
| $w_{21}^1(\text{new}) = w_{21}^1 - \eta \frac{\partial L}{\partial w_{21}^1}$ |                                                                               | $b_{21}(\text{new}) = b_{21} - \eta \frac{\partial L}{\partial b_{21}}$ |
| $w_{22}^1(\text{new}) = w_{22}^1 - \eta \frac{\partial L}{\partial w_{22}^1}$ |                                                                               |                                                                         |

Types of Gradient descent ->
1. Stochastic 
2. Batch
3. Mini-Batch

1 epoch : Each sample has gone thrpugh one pass of forward and backward propogation.
'm' -> number of samples in a dataset
if we have 50 -> m's , then we use 10 epochs means -> 50 *10 = 500 times 

1. Stochastic -> 
1. add a shuffle parameter before selection of the records
2. improved model performace beacause of the frequent updation of weights and biases
3. updating the weights based on just one sample can result in fluctuations in loss value.

2.Batch gradient descent ->
we use all the records in the dataset for forward propogation, use the aggregated loss value to update the weights.
more stable and less frequent updates 
i.e 50 samples, 10 epochs we will be updating the weights just 10 times.
Advantages ->
 computational efficiency, Stable Error gradient, Faster convergence
Dis ->
would need several epochs for training, requires the full data memory everytime, less scalability.

3. Mini-Batch gradient descent ->
Divides the training dataset into subsets
Then, forward propogation on the subset, loss function, updates the weights of the model based on the subset.
i.e 50 samples 10 batches -> 5 subsets, 5 epochs = 25 times
Note : Mini batch size of 2^n is preffered


1. Stochastic Gradient Descent ->
Implementation on code ->
import torch.optic as optim 
optimizer = optim.SGD(model.parameter(), lr = 0.001)

torch.optim is PyTorch’s built-in optimization module. It provides many optimizers (like SGD, Adam, RMSprop)
used for training neural networks via gradient descent.

optim.SGD is an implementation of Stochastic Gradient Descent (SGD)

loss.backwards()   # Compute new gradients
optimizer.step()   # Update weights
model[0].weight
model[2].weight

from torch.utils.data import TensorDataset, DataLoader
train_data = TensorDataset(X_train_tensor, y_train_tensor)
model = nn.Sequential(
nn.Linear(2,2)       ->  # input layer: 2 features to 2 neurons
nn.Sigmoid(),        ->  # activation function
nn.Linear(2,1)       ->  # output layer: 2 neurons to 1 output 
)

from torch.nn import MSEloss
criterion = MSELoss()

train_loss = criterion(model(X_train_tensor), y_train_tensor).item()
test_loss = criterion(model(X_test_tensor), y_test_tensor).item()
printf(f'Without training :\n Train Loss : {train_loss : .4f}, Test Loss : {test_loss:.4f}')

model(X_train_tensor)[:5]

Stochastic Gradient Descent ->
import torch
from torch.utils.data import DataLoader

# Create DataLoader
train_loader = DataLoader(train_data, batch_size=1, shuffle=True)

# Training loop
num_epochs = 10
for epoch in range(num_epochs):
    model.train()  # Set model to training mode
    for X_batch, y_batch in train_loader:
        # Forward pass
        pred = model(X_batch)
        loss = criterion(pred, y_batch)

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Evaluate loss after each epoch
    model.eval()  # Set model to evaluation mode
    with torch.no_grad():
        train_loss = criterion(model(X_train_tensor), y_train_tensor).item()
        test_loss = criterion(model(X_test_tensor), y_test_tensor).item()

    print(f'Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Test Loss = {test_loss:.4f}')


DataLoader(...)	 -> Creates an iterable loader over your training dataset.

optimizer.zero_grad() ->
It resets (clears) the gradients of all model parameters before computing new gradients during backpropagation.

with torch.no_grad(): ->
A context manager that temporarily disables gradient tracking.
During evaluation or inference, you don’t need gradients — you’re not calling loss.backward() or optimizer.step().
uses ->
Disabling gradient tracking:
Saves memory
Speeds up computation


2. Batch Gradient Descent ->
then we reinitialize the code ->
import torch
import torch.nn as nn
import torch.optim as optim

# Reinitialize the model
model = nn.Sequential(
    nn.Linear(2, 2),
    nn.Sigmoid(),
    nn.Linear(2, 1)
)

# Define optimizer with learning rate
optimizer = optim.SGD(model.parameters(), lr=0.01)


import torch
from torch.utils.data import DataLoader

# Create DataLoader with all 800 samples in a single batch
train_loader = DataLoader(train_data, batch_size=800, shuffle=True)

# Training loop for 1000 epochs
num_epochs = 1000
for epoch in range(num_epochs):
    model.train()  # Set model to training mode

    for X_batch, y_batch in train_loader:
        # Forward pass
        pred = model(X_batch)
        loss = criterion(pred, y_batch)

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Print losses every 100 epochs
    if (epoch + 1) % 100 == 0:
        model.eval()  # Set model to evaluation mode
        with torch.no_grad():
            train_loss = criterion(model(X_train_tensor), y_train_tensor).item()
            test_loss = criterion(model(X_test_tensor), y_test_tensor).item()
        print(f'Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Test Loss = {test_loss:.4f}')

Note ->
 if (epoch + 1) % 100 == 0:
        model.eval()  # Set model to evaluation mode

model.eval()	            ->      Switch to evaluation mode
(epoch + 1) % 100 == 0	  ->      Checkpoint every 100 epochs
so we print the loss for train and test after every 100 epochs.


3. Mini-Batch ->

# Reinitialize the model
model = nn.Sequential(
    nn.Linear(2, 2),
    nn.Sigmoid(),
    nn.Linear(2, 1)
)

# Define optimizer with learning rate
optimizer = optim.SGD(model.parameters(), lr=0.01)

from torch.utils.data import DataLoader

# Assuming model, criterion, optimizer, train_data,
# X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor are defined

# Create DataLoader with batch size 64
train_loader = DataLoader(train_data, batch_size=64, shuffle=True)

# Training loop for 500 epochs
for epoch in range(500):
    model.train()  # Set model to training mode

    for X_batch, y_batch in train_loader:
        # Forward pass
        pred = model(X_batch)
        loss = criterion(pred, y_batch)

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Print losses every 50 epochs
    if (epoch + 1) % 50 == 0:
        model.eval()  # Set model to evaluation mode
        with torch.no_grad():
            train_loss = criterion(model(X_train_tensor), y_train_tensor).item()
            test_loss = criterion(model(X_test_tensor), y_test_tensor).item()
        print(f'Epoch {epoch + 1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')

| Method                      | Batch Size       | Samples (N) | Epochs (E) | Batches per Epoch | Total Updates (Batches × Epochs) |
| --------------------------- | ---------------- | ----------- | ---------- | ----------------- | -------------------------------- |
| Stochastic Gradient Descent | 1                | 800         | 10         | 800               | 8,000                            |
| Batch Gradient Descent      | 800 (full batch) | 800         | 1000       | 1                 | 1,000                            |
| Mini-Batch Gradient Descent | 64               | 800         | 500        | 13                | 6,500                            |



Other optimization Techniques ->
1.Stochastic Gradient Descent with momentum  ->
Accelerates convergence by adding a fraction of the previous update to the current update.
Helps escape local minima and smooths oscillations.(reach the global minima easily)

2.Nesterov Accelerated Gradient (NAG) / Nesterov Momentum ->
A variant of Momentum that looks ahead by calculating the gradient at the approximate future position of the parameters.
Often provides faster convergence than standard momentum.

3.AdaGrad (Adaptive Gradient Algorithm) ->
Adapts learning rates individually for each parameter based on past gradients.
Performs larger updates for infrequent parameters, smaller updates for frequent ones.
Can suffer from aggressive, monotonically decreasing learning rates.

4.RMSProp (Root Mean Square Propagation) ->
Fixes AdaGrad’s decaying learning rate issue by using a moving average of squared gradients.
Keeps the learning rate adaptive and balanced over time.

5.Adam (Adaptive Moment Estimation) ->
Combines ideas from Momentum and RMSProp.
Keeps running averages of both gradients and their squared values to adapt learning rates for each parameter.
Generally considered one of the best default optimizers — fast convergence and robust.


1.Stochastic Gradient Descent with momentum ->
Implementation : optimizer = optim.SGD(model.parameter(), lr = 0.001, momentum =0.9)

1. Compute Gradient at Current Weights : g_t = ∇L(w_t)
2. Update the Velocity (Moving Average of Gradients) : v_t = β * v_(t-1) + (1 - β) * g_t
3. Update the Weights : w_(t+1) = w_t - η * v_t
| Symbol    | Meaning                       |
| --------- | ----------------------------- |
| `g_t`     | Gradient at current weights   |
| `v_t`     | Momentum term (velocity)      |
| `β`       | Momentum decay factor (0.9\~) |
| `η`       | Learning rate                 |
| `w_t`     | Current weights               |
| `w_(t+1)` | Updated weights               |

Assume:
Initial weight 0 = 4 
Learning rate 𝜂 = 0.1
Momentum 𝛾 = 0.9
Initial velocity 𝑣 =0
Loss: 
L(θ)=θ^2 ⇒∇L=2θ

🔁 Step-by-Step:

Step 1:
g=2⋅4=8
v=0.9⋅0 + 0.1⋅8=0.8
𝜃 = 4 − 0.8 = 3.2

Step 2:
g=2⋅3.2 = 6.4
v=0.9⋅0.8 + 0.1⋅6.4=1.36
θ=3.2−1.36=1.84

Step 3:
g= 2⋅1.84 = 3.68
v = 0.9⋅1.36 + 0.1⋅3.68=1.592
𝜃 = 1.84−1.592= 0.248


2.Nesterov Accelerated Gradient (NAG) / Nesterov Momentum :
Implementation : optimizer = optim.SGD(model.parameter(), lr = 0.001, momentum =0.9, nesterov =True)

1. Lookahead Step -> Estimate future weights using current velocity: W_lookahead = W_t - γ * v_(t-1)
2. Compute Gradient at Lookahead Weights -> Calculate the gradient at the lookahead position: g_t = ∇L(W_lookahead)
3. Update Velocity (Momentum Term) : v_t = β * v_(t-1) + (1 - β) * g_t
4. Update Weights (Parameters) : W_(t+1) = W_t - η * v_t
| Symbol        | Meaning                                                                                   |
| ------------- | ----------------------------------------------------------------------------------------- |
| `W_t`         | Current weights at time step `t`                                                          |
| `W_lookahead` | Lookahead weights estimated by moving from current weights along the previous velocity    |
| `g_t`         | Gradient of the loss function computed at the lookahead weights (`∇L(W_lookahead)`)       |
| `v_t`         | Velocity (momentum term), a moving average of past gradients                              |
| `β`           | Momentum decay factor (typically \~0.9), controls how much past gradients affect velocity |
| `γ`           | Momentum factor, often same as `β`, scales the lookahead step                             |
| `η`           | Learning rate, controls step size during weight update                                    |
| `W_(t+1)`     | Updated weights after applying the momentum-based gradient step                           |

3.AdaGrad (Adaptive Gradient Algorithm)
Parameters that get frequent updates receive smaller learning rates.
Parameters that are rarely updated receive larger learning rates.
Accumulate squared gradients for each parameter i: G_t,i = G_(t-1),i + (g_t,i)^2

Where:
G_t,i is the sum of squared gradients for parameter i up to time t
g_t,i is the gradient of the loss with respect to parameter i at time t

Update the parameter: θ_t+1,i = θ_t,i - (η / (sqrt(G_t,i) + ε)) * g_t,i

Where:
η is the learning rate
ε is a small value to prevent division by zero (e.g., 1e-8)
sqrt(G_t,i) scales the learning rate based on past gradients

Time-based decay

Reduces the learning rate as a function of time step t:  η_t = η_0 / (1 + decay_rate * t) -> decay rate is fixed to 0.1(in pytorch)
0: initial learning rate
t: current epoch or iteration
Decreases slowly over time

4.RMSProp (Root Mean Square Propagation)

1. Update moving average of squared gradients: E[g²]_t = γ * E[g²]_(t−1) + (1 − γ) * (g_t)²
2. Update parameters: θ = θ − (η / sqrt(E[g²]_t + ε)) * g_t
So, learning rate is divided by the RMS (Root Mean Square) of recent gradients.

5.Adam (Adaptive Moment Estimation)

Adam dynamically adjusts the learning rate for each parameter, using:
A moving average of gradients (1st moment)
A moving average of squared gradients (2nd moment)

Update biased first moment estimate (momentum): m_t = β1 * m_(t-1) + (1 - β1) * g_t
Update biased second raw moment estimate (squared gradients): v_t = β2 * v_(t-1) + (1 - β2) * (g_t)^2
Bias correction (because mt and vt are initialized at 0):
m̂_t = m_t / (1 - β1^t)
v̂_t = v_t / (1 - β2^t)
Parameter update: θ = θ - η * m̂_t / (sqrt(v̂_t) + ε)


