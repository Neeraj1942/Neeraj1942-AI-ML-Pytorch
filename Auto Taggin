On stack exchange dataset -> http://stackexchange.com/
Quality -> the number of uploads 
Dividing each question into ->
a. title
b. question
c. tags

Business Problem: Untagged questions or questions with irrelevant tags lead to a large number of unanswered questions. 
The experts find it difficult to discover the right questions, and the users who post questions don't get timely responses. 
This situation leads to a drop in the level of user-engagement on the platform.

Input           : Text data
Target Variable : Multiple Tags
(Note: the major difference is in binary and multiclass classification we have one input to one target variable, 
but in this case - we can have one question(input) taggged with multiple tags (target variable).

Code ->

pd.read_csv('Questions.csv', encoding='latin-1')
we use this to -> the encoding='latin-1' is used to avoid errors caused by special characters in the dataset.

for the Tags.csv we dont use because, it has no errros, we can use latin1 here without any issues
the default is default UTF-8 -> 
| Feature                                   | UTF-8           | Latin-1 (`latin-1`)     |
| ----------------------------------------- | --------------- | ----------------------- |
| Default in Python/pandas                  | ✅ Yes           | ❌ No                    |
| Supports emojis                           | ✅ Yes           | ❌ No                    |
| Supports all languages                    | ✅ Yes           | ❌ Only Western European |
| File size                                 | Smaller usually | Larger                  |
| Crashes on wrong characters               | ❌ Yes           | ✅ Never crashes         |
| Good for messy Kaggle/StackOverflow text? | Sometimes       | ✅ Yes                   |
| Safe for simple text                      | ✅ Yes           | ✅ Yes                   |

def cleaner(text):

  # take off html tags
  text = BeautifulSoup(text).get_text() -> beautifulSoup helps wipe out all the html tags 
  
  # fetch alphabetic characters
  text = re.sub("[^a-zA-Z]", " ", text) -> This line removes numbers, punctuation, and special characters replaces them with space 

  # convert text to lower case
  text = text.lower() -> changes all the aplhabets to lower case

  # split text into tokens to remove whitespaces
  tokens = text.split() -> splits the sentences into tokens , like -> ['Machine', 'learning', 'is', 'fun']

  return " ".join(tokens) -> join isused to set it back into sentences -> machine learning is fun

# call preprocessing function
questions_df['cleaned_text'] = questions_df['Body'].apply(cleaner) -> to call the function and apply it to the preprocessing
now the questions_df['Body'] is same but the cleaned version is named questions_df['cleaned_text']


2. Merging the tags ->
# count of unique tags
len(tags_df['Tag'].unique())

tags_df['Tag'].value_counts() -> gives you a frequency count of each tag in your Tags.csv dataset

# remove "-" from the tags
tags_df['Tag']= tags_df['Tag'].apply(lambda x:re.sub("-"," ",x)) 
-> replaces all the '-' symbols with spaces to stop error , the lambda function helps to apply this to every row in the dataset

# group tags Id wise
tags_df = tags_df.groupby('Id').apply(lambda x:x['Tag'].values).reset_index(name='tags')
tags_df.head()
-> Groups the dataframe by Id and collects all Tag values for each Id into a single array in a new column called tags.

# merge tags and questions
df = pd.merge(questions_df,tags_df,how='inner',on='Id') 
-> Merges questions_df and tags_df on the Id column, keeping only rows with matching Ids and adding the grouped tags to each question.

df = df[['Id','Body','cleaned_text','tags']]
df.head() -> Selects and keeps only the columns Id, Body, cleaned_text, and tags in the dataframe df.

3. Dateset preparation ->
freq = {'data science': 2, 'ai': 3, 'ml': 1}
print(freq.keys())
output : dict_keys(['data science', 'ai', 'ml'])

in dictionaries -> items = keys : value

# sort the dictionary in descending order
freq = dict(sorted(freq.items(), key=lambda x:x[1],reverse=True)) 
-> Sorts the dictionary freq by its values in descending order and returns a new dictionary with keys ordered from highest to lowest value.
here ( x : x[1] gives the vlaues to the dictionary)

# Top 10 most frequent tags
common_tags = list(freq.keys())[:10]
common_tags -> gives the top 10 frequencies in the list

x=[]
y=[]
for i in range(len(df['tags'])):
  
  temp=[]
  for j in df['tags'][i]:
    if j in common_tags:
      temp.append(j)

  if(len(temp)>1):
    x.append(df['cleaned_text'][i])
    y.append(temp)
-> this codes checks if the tags are in the common_tags, then adds it to the y
-> then we check the assign all the questions(cleaned_text) in the x with their tags (out of 10)
Note :
x.append(df['cleaned_text'][i]) → a list of cleaned_text
y.append(temp) → a list of tags

Importing ->
from sklearn.preprocessing import MultiLabelBinarizer
mlb = MultiLabelBinarizer()
y = mlb.fit_transform(y)
y.shape
-> this converts the y into a converts each sample (inner list) into a binary vector (0 or 1 for presence/absence of tags).

y[0,:] -> array([0, 0, 0, 0, 0, 0, 1, 0, 0, 1])

mlb.classes_ -> array(['classification', 'distributions', 'hypothesis testing',
       'logistic', 'machine learning', 'probability', 'r', 'regression',
       'self study', 'time series'], dtype=object)

mlb.classes_ ->
The trailing underscore _ is part of scikit-learn’s naming convention.
Attributes with _ at the end are set during fitting.
mlb.classes_ is an attribute, not a method.

from sklearn.model_selection import train_test_split
x_tr,x_val,y_tr,y_val=train_test_split(x, y, test_size=0.2, random_state=0,shuffle=True)


4. Text Representation ->
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences 

#prepare a tokenizer
x_tokenizer = Tokenizer() 
x_tokenizer.fit_on_texts(x_tr)

explanation ->
from keras.preprocessing.text import Tokenizer
x_tokenizer = Tokenizer()
-> Tokenizer is a text preprocessing utility.
Its job is to convert text into numbers that neural networks can work with.
Essentially, it builds a vocabulary of all words in your text corpus and assigns a unique integer to each word.

x_tokenizer.word_index ->
sample -> {'the': 1,
 'i': 2,
 'to': 3,
 'a': 4,
 'of': 5,
 'instances': 1000,
 ...}

x_tokenizer.fit_on_texts(x_tr) -> .fit_on_texts() learns the vocabulary from your text data x_tr.

len(x_tokenizer.word_index) -> 25312 

thresh = 3

cnt=0
for key,value in x_tokenizer.word_counts.items():
  if value>=thresh:
    cnt=cnt+1
print(cnt) -> 12574 
Explanation : Counts how many words in the tokenizer’s vocabulary appear 3 or more times in the training texts.

# prepare the tokenizer again
x_tokenizer = Tokenizer(num_words=cnt, oov_token='unk')

# prepare vocabulary
x_tokenizer.fit_on_texts(x_tr)

Tokenizer(num_words=cnt, oov_token='unk') ->
num_words -> The maximum number of words to keep in the tokenizer’s vocabulary.
oov_token -> Out-of-vocabulary” token — a special token for words not in the vocabulary.
here 'unk' is a new token assigned to the the dataset, with value 1, (unk :1).
if our num_words = 10 , then only the values with the value 9 are used and the rest are replaced with 'unk'.

x_tokenizer.fit_on_texts(x_tr) ->
fit_on_texts -> 
Goes through all sentences in x_tr.
Counts how often each word appears.
Sorts words by frequency.
Creates a word → index mapping (word_index).

example output -> {'ai': 1, 'i': 2, 'love': 3, 'loves': 4, 'me': 5}

texts_to_sequences -> another method to convert the texts into numerical data in keras tokenizer
code -> x_tr_seq = x_tokenizer.texts_to_sequences(x_tr)
example output -> {'ai': 1, 'i': 2, 'love': 3, 'loves': 4, 'me': 5}
the arrays would be -> 
| Sentence      | Tokenized Output |
| ------------- | ---------------- |
| "I love AI"   | `[2, 3, 1]`      |
| "AI loves me" | `[1, 4, 5]`      |

Padding -> Padding works on number sequences, not words. 
So we must first convert text → tokens → number sequences, then pad.

| Step                      | Why it's needed         | Example                    |
| ------------------------- | ----------------------- | -------------------------- |
| 1. Clean Text             | remove noise            | "I love AI!" → "i love ai" |
| 2. Tokenize Text          | split into words        | ["i", "love", "ai"]        |
| 3. Convert to Sequences ✅ | convert words → numbers | [2, 5, 1]                  |
| 4. Pad Sequences ✅        | make equal length       | [2, 5, 1, 0, 0]            |

function used is -> pad_sequences
usage : x_tr_seq = pad_sequences(x_tr_seq,  padding='post', maxlen=max_len)
maxlen is already defined
padding = 'post' means adding the 0's at the end
x_tr_seq : converted text data into numerical 
Example - > [[2, 3, 4], [4, 5, 6, 7]]
  becomes -> 
[[2 3 4 0 0 0]
 [4 5 6 7 0 0]]

Note on padding ->
| Padding Type      | Pad Location(LESS) | Truncation Side(MORE)   | Example (Max Len = 10, Sentence = 9 words)          |
|  ---------------- | -----------------  | ---------------------   | --------------------------------------------------- |
|  **Post-padding** | End of sequence    | Beginning of sequence   | "The quick brown fox jumps over the lazy dog [PAD]" |
|  **Pre-padding**  | Start of sequence  | End of sequence         | "[PAD] The quick brown fox jumps over the lazy dog" |


Note: If we have more than the max_length in the converted data, then the last after 100 are removed or truncated !

Note: as we have used a new token (oov_token ='unk') we have to add this to our vocabulary size
x_voc_size = x_tokenizer.num_words + 1 (done to add the padding token : '0')

first input question (x) is -> x_tr_seq[0] (we get 100 number; dtype=int32)


Model Building :
Embedding() -> categorical integers (like word indices) into dense vectors of fixed size
embedding_layer = Embedding(
    input_dim=vocab_size,     # total number of unique words + 1
    output_dim=embedding_dim, # size of the embedding vector
    input_length=max_len      # length of input sequences
)

Our Code : model.add(Embedding(x_voc_size, 50, input_shape=(max_len,), mask_zero=True))
Explanation ->
| Parameter / Part         | Meaning                                                               |
| ------------------------ | --------------------------------------------------------------------- |
| `Embedding`              | Layer that converts word indices into dense vectors (word embeddings) |
| `x_voc_size`             | Vocabulary size (number of unique words + 1 for padding)              |
| `50`                     | Dimension of the embedding vector for each word                       |
| `input_shape=(max_len,)` | Shape of input sequences (each sequence has `max_len` tokens)         |
| `mask_zero=True`         | Tells the layer to ignore padding (`0`) during training               |


SimpleRNN layer ->
model.add(SimpleRNN(128,activation='relu'))
| Feature    | Explanation                                                                                              |
| ---------- | -------------------------------------------------------------------------------------------------------- |
| Recurrent  | Processes data **step by step**, maintaining a hidden state that carries information from previous steps |
| Input      | Expects **3D tensor**: `(batch_size, timesteps, features)`                                               |
| Output     | Can return **the last output** or **the full sequence**                                                  |
| Activation | Typically `tanh` by default                                                                              |
| Limitation | Cannot capture **long-term dependencies** well (vanishing gradient problem)                              |

| Parameter / Part    | Meaning                                                                                                                                            |
| ------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |
| `SimpleRNN`         | A type of recurrent neural network (RNN) layer that processes sequences step by step, maintaining a hidden “memory” of previous steps.             |
| `128`               | Number of **units** (or neurons) in the RNN layer. Each unit has its own hidden state, so the output at each timestep is a 128-dimensional vector. |
| `activation='relu'` | Activation function applied to the RNN units. `'relu'` introduces non-linearity, helping the model learn complex patterns.                         |

for the code the summary paramters are caculated like this - 
lengh is 12575 
| Layer              | Output Shape      | Parameters | Calculation                                                           |
| ------------------ | ----------------- | ---------- | --------------------------------------------------------------------- |
| **Embedding**      | `(None, 100, 50)` | 628,750    | `x_voc_size * embedding_dim = 12,575 * 50 = 628,750`                  |
| **SimpleRNN(128)** | `(None, 128)`     | 22,912     | `(input_dim + units) * units + units = (50 + 128)*128 + 128 = 22,912` |
| **Dense(128)**     | `(None, 128)`     | 16,512     | `input_dim * units + units = 128*128 + 128 = 16,512`                  |
| **Dense(10)**      | `(None, 10)`      | 1,290      | `input_dim * units + units = 128*10 + 10 = 1,290`                     |
| **Total**          | —                 | 669,464    | Sum of all parameters                                                 |

in simple rnn -> (input_dim + units) * units + units :
input_dim * units + units * units + units -> 
units * units because : The current input x_t → gives input_dim * units parameters
The previous hidden state h_{t-1} → each of the 128 neurons in h_t receives input from all 128 neurons in h_{t-1}

then we use optimzer and loss
then we create a ModelCheckpoint() 
mc = ModelCheckpoint("weights.best.hdf5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

Then train the model :
model.fit(x_tr_seq, y_tr, batch_size=128, epochs=10, verbose=1, validation_data=(x_val_seq, y_val), callbacks=[mc])

ModelPrediction ->
# load weights into new model
model.load_weights("weights.best.hdf5")

#predict probabilities
pred_prob = model.predict(x_val_seq)

Why do we again use model.load_weights() ->
For example:
Epoch 1 → validation loss = 0.5
Epoch 5 → validation loss = 0.3 (best)
Epoch 10 → validation loss = 0.35
If you just use the model after training, it will use the weights from epoch 10, 
which are slightly worse than the best ones.

So when we call model.load_weights() again after the 
training -> the weights stored at epoch 5 are used

now we predict the probability -> pred_prob[0] : we get ouput like ->
array([0.05661559, 0.01500756, 0.0809429 , 0.34682304, 0.14759117,
       0.00561154, 0.4962414 , 0.627706  , 0.0462845 , 0.24911207],
      dtype=float32)

then we create a threshold to check each one, to find the best one out of them all ->
#define candidate threshold values
threshold  = np.arange(0,0.5,0.01)
threshold

then we use a method called classifiy(pred_prob,thresh) ->
to calculate each of the 10 probs to 0's or 1's

y_true = np.array(y_val).ravel() -> to convert this into numpy array to flattened numpy 1-d array :
example ->  y_val = [[0,1,0,1], [1,0,0,1], [0,0,1,0]]
after .ravel() : stilla numpy array but 1-d
array([[0, 1, 0, 1],
       [1, 0, 0, 1],
       [0, 0, 1, 0]])


then we use that use the classify() function to check for all the threshold.
then using .ravel() we convert the 10 probs into 1-d numpy array and store the ->
y_val flattened to y_true 
y_pred_seq flattened to y_pred 

then we use metrics to calculate the f1 score for ->
 score.append(metrics.f1_score(y_true,y_pred)) , and store it in score[]

then find theo ne with the threshold with the max score ->
opt = threshold[score.index(max(score))]

y_pred_seq = classify(pred_prob, opt)  # 2D list
y_pred = np.array(y_pred_seq).ravel()  # flatten to 1D array

then we flatten the new one with using the best threshold (opt)

print the classification_report ->
print(metrics.classification_report(y_true,y_pred))

then we convert the binary arays back into the initial label formats 
inverse_transform converts the binary array back into the original label format

y_pred = mlb.inverse_transform(np.array(y_pred_seq))
y_true = mlb.inverse_transform(np.array(y_val))

df = pd.DataFrame({'comment':x_val,'actual':y_true,'predictions':y_pred})
-> this helps to create a oandas dataframe with the new acjived values


Inference ->
In machine learning and deep learning, inference simply means using a 
trained model to make predictions on new data

def predict_tag(comment):  
  text=[]

  #preprocess  
  text = [cleaner(comment)]

  #convert to integer sequences
  seq = x_tokenizer.texts_to_sequences(text)

  #pad the sequence
  pad_seq = pad_sequences(seq,  padding='post', maxlen=max_len)

  #make predictions
  pred_prob = model.predict(pad_seq)
  classes = classify(pred_prob,opt)[0]
  
  classes = np.array([classes])
  classes = mlb.inverse_transform(classes)  
  return classes


Example ->
Input: "How do I merge two dataframes in pandas?"

Clean: "how do i merge two dataframes in pandas"

Tokenize → [12, 5, 7, 120, 78, 14, 202]

Pad → [12, 5, 7, 120, 78, 14, 202, 0, ..., 0]

Predict → [0.12, 0.01, 0.03, 0.87, 0.02, 0.05, 0.60, ...]

Apply threshold → [0, 0, 0, 1, 0, 0, 1, ...]

Inverse transform → ('python', 'pandas')

Output: predicted tags ✅

Incase you want to use all the 1+ questions and predict the tags we can replace ->


  pad ->  pad_seq = pad_sequences(seq, padding='post', maxlen=max_len)

  predict -> pred_prob = model.predict(pad_seq)

  apply threshold -> classes_array = np.array(classify(pred_prob, opt))

  inverse transform ->  predicted_tags = mlb.inverse_transform(classes_array)



