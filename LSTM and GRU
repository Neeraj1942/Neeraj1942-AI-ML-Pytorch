LSTM (Long Short-Term Memory)->
| Gate            | Purpose                                                          |
| --------------- | ---------------------------------------------------------------- |
| **Forget Gate** | Decides what information from the previous state to **discard**. |
| **Input Gate**  | Decides what new information to **store** in the cell state.     |
| **Output Gate** | Decides what information to **output** from the current cell.    |

| Component / Gate            | Formula                                              | Activation | Purpose                                                                   |
| --------------------------- | ---------------------------------------------------- | ---------- | ------------------------------------------------------------------------- |
| **Forget Gate (f_t)**       | ( f_t = σ(W_f x_t + U_f h_{t-1} + b_f) )        | Sigmoid    | Decide which information to **forget** from previous cell state (C_{t-1}) |
| **Input Gate (i_t)**        | ( i_t = σ(W_i x_t + U_i h_{t-1} + b_i) )        | Sigmoid    | Decide which new information to **store** in cell state                   |
| **Candidate Memory (~C_t)** | ( \tilde{C}*t = \tanh(W_c x_t + U_c h*{t-1} + b_c) ) | Tanh       | Generate **candidate values** to add to cell state                        |
| **Cell State Update (C_t)** | ( C_t = f_t * C_{t-1} + i_t * \tilde{C}_t )          | Linear     | Update **cell memory** by combining old and new info                      |
| **Output Gate (o_t)**       | ( o_t = σ(W_o x_t + U_o h_{t-1} + b_o) )        | Sigmoid    | Decide which information from cell state to **output**                    |
| **Hidden State (h_t)**      | ( h_t = o_t * \tanh(C_t) )                           | Tanh       | Final **output** of LSTM at current time step                             |


| Terminology            | Meaning                                                                                                                   |
| ---------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| **x_t**                | Input vector at current time step **t**. For NLP, this could be word embeddings; for time series, the current value.      |
| **h_{t-1}**            | Hidden state (output) from the **previous time step**. Carries short-term information to the current step.                |
| **C_{t-1}**            | Cell state from the previous time step. Stores **long-term memory**.                                                      |
| **f_t**                | Forget gate output (values between 0 and 1). Determines **how much of C_{t-1} to keep or discard**.                       |
| **i_t**                | Input gate output (values between 0 and 1). Determines **how much of the new candidate memory (~C_t) to write into C_t**. |
| **~C_t**               | Candidate memory values (between -1 and 1) that are proposed to **update the cell state**.                                |
| **C_t**                | Updated cell state. Combines forgotten old memory and added new memory. Represents **long-term memory at time t**.        |
| **o_t**                | Output gate (values between 0 and 1). Controls **how much of the cell state to expose as hidden state**.                  |
| **h_t**                | Hidden state at time t. This is the **output of the LSTM cell** and is passed to the next layer or time step.             |
| **W_f, W_i, W_c, W_o** | Weight matrices applied to the **current input x_t** for each gate. Learned during training.                              |
| **U_f, U_i, U_c, U_o** | Recurrent weight matrices applied to the **previous hidden state h_{t-1}** for each gate.                                 |
| **b_f, b_i, b_c, b_o** | Bias vectors for each gate. Help shift the activation function and improve learning.                                      |
| **σ (sigmoid)**        | Activation function mapping any value to **0–1**. Used for gates to represent a proportion (keep, add, output).           |
| **tanh**               | Hyperbolic tangent activation function mapping any value to **-1 to 1**. Used for candidate memory and scaling output.    |                                                        |

| Gate Value  | Gate Name   | Effect                                          |
| ----------- | ----------- | ----------------------------------------------- |
| ( f_t = 0 ) | Forget Gate | Forget **all previous memory**                  |
| ( f_t = 1 ) | Forget Gate | Keep **all previous memory**                    |
| ( i_t = 0 ) | Input Gate  | **Block new information**, no update            |
| ( i_t = 1 ) | Input Gate  | **Allow full new information** to update memory |
| ( o_t = 0 ) | Output Gate | **Hide memory**, no output from LSTM            |
| ( o_t = 1 ) | Output Gate | **Show full memory** as output                  |

Important obervation in lstm ->
| Question                                                              | Answer                                                                                                                           |
| --------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| How many different types of gates and weight matrices does LSTM have? | LSTM has **3 gates** (forget, input, output) and **8 weight matrices** ((W_f, W_i, W_o, W_c, U_f, U_i, U_o, U_c)).               |
| Which component acts like a memory in LSTM?                           | The **cell state (C_t)** acts as **long-term memory**.                                                                           |
| Which activation function is common in all the gates in LSTM?         | **Sigmoid** is used in all three gates.                                                                                          |
| Can we replace sigmoid with tanh in all the gates in LSTM?            | **No.** Sigmoid outputs 0–1, controlling information flow, while tanh outputs -1 to 1. Tanh cannot replace sigmoid in the gates. |
| The cell state is independent of which of the states                  | Both input and forget gate |

Breaking the concept down ->
C(t-1) , C(t) are the -> Long term memory of the LSTM
H(t-1) , H(t) are the -> Short term memory of the LSTM

the current cell state is a equation with the both the previous current state cell and the new information.
cell state acts as a memory for the LSTM


GRU (Gates Recurrent Unit) ->
No seperate cell state C(t)

Only 2 gates in a gru :
1. Reset Gate (short-term memory)
2. Update Gate (long-term memory)

| Gate                 | Purpose                                         |
| -------------------- | ----------------------------------------------- |
| **Update Gate (zₜ)** | Decides **how much past information to keep**   |
| **Reset Gate (rₜ)**  | Decides **how much past information to forget** |

| Step                          | Formula                                                          | Meaning                           |
| ----------------------------- | ---------------------------------------------------------------- | --------------------------------- |
| **1. Update Gate**            | ( z_t = \sigma(W_z x_t + U_z h_{t-1} + b_z) )                    | Controls memory retention         |
| **2. Reset Gate**             | ( r_t = \sigma(W_r x_t + U_r h_{t-1} + b_r) )                    | Controls forgetting of old memory |
| **3. Candidate Hidden State** | ( \tilde{h}*t = \tanh(W_h x_t + U_h (r_t \odot h*{t-1}) + b_h) ) | Creates new memory                |
| **4. Final Hidden State**     | ( h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t )        | Final output memory               |

| Symbol            | Meaning                          |
| ----------------- | -------------------------------- |
| ( x_t )           | Input at time t                  |
| ( h_{t-1} )       | Previous hidden state            |
| ( \sigma )        | Sigmoid activation               |
| ( W_z, W_r, W_h ) | Weight matrices for input        |
| ( U_z, U_r, U_h ) | Weight matrices for hidden state |
| ( b_z, b_r, b_h ) | Bias terms                       |
| ( \odot )         | Element-wise multiplication      |

| Feature         | GRU     | LSTM                   |
| --------------- | ------- | ---------------------- |
| Number of gates | 2       | 3                      |
| Parameters      | Fewer   | More                   |
| Speed           | Faster  | Slower                 |
| Accuracy        | Similar | Good for complex tasks |
| Architecture    | Simple  | Slightly complex       |

| Gate Value  | Effect                               |
| ----------- | ------------------------------------ |
| ( r_t = 0 ) | Reset memory → ignore past (h_{t-1}) |
| ( r_t = 1 ) | Use full past memory in update       |
| ( z_t = 0 ) | Keep old memory, ignore new input    |
| ( z_t = 1 ) | Replace old memory completely        |

Note : 
| Model    | Gates                                                         | Recurrent Connections (U)                             | Description                                            |
| -------- | ------------------------------------------------------------- | ----------------------------------------------------- | ------------------------------------------------------ |
| **GRU**  | 2 gates (Reset (r_t), Update (z_t))                           | **3 recurrent weight matrices**: (U_r, U_z, U_h)      | One recurrent connection per gate + candidate state    |
| **LSTM** | 3 gates (Forget (f_t), Input (i_t), Output (o_t)) + candidate | **4 recurrent weight matrices**: (U_f, U_i, U_o, U_c) | More recurrent feedback because it tracks a cell state |

Using LSTM for autotagging issue ->

Everything remains the same as the RNN systems, but in the while defining the model we do ->
#sequential model
model = Sequential()

#embedding layer
model.add(Embedding(x_voc_size, 50, trainable = True, input_shape=(max_len,),mask_zero=True))

#lstm 
model.add(LSTM(128))   -> only this is the difference

#dense layer
model.add(Dense(128,activation='relu')) 

#output layer
model.add(Dense(10,activation='sigmoid'))

| Layer      | Parameter Formula                     | Param Count |
| ---------- | ------------------------------------- | ----------- |
| Embedding  | vocab_size × embed_dim                | 628,750     |
| LSTM       | 4 × ((input + units) × units + units) | 91,648      |
| Dense(128) | input × units + bias                  | 16,512      |
| Dense(10)  | input × units + bias                  | 1,290       |

LSTM gate :
4 × ((input + units) × units + units) -> 4 gates so - and it learns from the previous data
4 * (input * units + units * units + units) 
Explanation of the 4 gates ->
| Gate Name       | Symbol          | Purpose                                        |
| --------------- | --------------- | ---------------------------------------------- |
| Forget Gate     | ( f_t )         | Decides what part of previous memory to forget |
| Input Gate      | ( i_t )         | Decides what new information to store          |
| Candidate State | ( \tilde{c}_t ) | Creates new candidate memory                   |
| Output Gate     | ( o_t )         | Decides what part of memory to output          |


How to improve the hyperparameters in the LSTM model ->
1. You can add more LSTM layers
2. You can pass pre-trained word embeddings
3. You can play with different optimizers
4. Try different input sequence length


For GRU model ->
model = Sequential()
model.add(Embedding(x_voc_size, 50, trainable=True, input_shape=(max_len,), mask_zero=True))
model.add(GRU(128))   # <-- Changed to GRU
model.add(Dense(128, activation='relu'))
model.add(Dense(10, activation='sigmoid'))


| Layer      | Parameter Formula                         | Param Count |
| ---------- | ----------------------------------------- | ----------- |
| Embedding  | vocab_size × embed_dim                    | 628,750     |
| **GRU**    | 3 × ((input_dim + units) × units + units) | 68,736      |
| Dense(128) | input_dim × units + bias                  | 16,512      |
| Dense(10)  | input_dim × units + bias                  | 1,290       |

Here we have 3 states for the gru -> 3 × ((input_dim + units) × units + units)
: 3 * (input_dim * units + units * units + units)
Note : the 3 gates are -> 
| Gate Name       | Symbol          | Purpose                                     |
| --------------- | --------------- | ------------------------------------------- |
| Update Gate     | ( z_t )         | Decides how much past information to keep   |
| Reset Gate      | ( r_t )         | Decides how much past information to forget |
| Candidate State | ( \tilde{h}_t ) | New memory content to be added              |





