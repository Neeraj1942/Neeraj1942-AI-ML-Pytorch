3 types of layers ->
1. Layer 1  -> Input Layer  -> Takes input features
2. Layer 2  -> Hidden Layer -> Applies weights and biases, then passes through an activation function
3. Layer 3  -> Output Layer -> Transforms hidden layer output into a final prediction.

Then,
Fire or No Fire -> for inputs like x1 ,x2 ->
x1 + x2 > threshold -> Fire 
x1 + x2 < threshold -> No Fire
 
x1 + x2 - threshold > 0
x1 + x2 - threshold < 0

when (-threshold = bias)


Mainly there are 3 steps ->
1. Bias -> Bias is an extra parameter added to the weighted sum.
It helps the network make predictions even when all inputs are zero.

x1 + x2 + bias > 0  -> Fire
x1 + x2 + bias > 0  -> No Fire

2. Weights -> Weights are the learnable parameters that control the strength of the connection between neurons in one layer to the next.
Each input is multiplied by a weight before being passed forward.

y(output) = wo + x0 + w1 + x1 + w2 + x2 +.......+ wn + xn , ( x is the input)

3. Activation Function -> An activation function is applied to the output z of a neuron to introduce non-linearity.
(If not applied, Without it, a neural network would just be a big linear function — which can't model complex patterns)
Examples ->
1. ReLU    -> Most common for hidden layers                    - [0,∞)
2. Sigmoid -> Binary classification output                     - (0,1)
3. Tanh    -> Similar to sigmoid, but outputs between -1 and 1 - (-1,1)
4. Softmax -> Multi-class classification output                - (0,1)->(0,1), and all outputs sum to 1

Now Unnderstanding,
Forward Propogation -> Forward propagation is how a neural network takes input 
(like a picture or some numbers), processes it, and gives an output (like a prediction).

This process of detailed in the forward direction from input to output ->
Now At each layer, the network performs two main steps:
1. Linear transformation: Multiply inputs by weights and add bias.
2. Non-linear activation: Apply an activation function to the result.



