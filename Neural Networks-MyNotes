3 types of layers ->
1. Layer 1  -> Input Layer  -> Takes input features
2. Layer 2  -> Hidden Layer -> Applies weights and biases, then passes through an activation function
3. Layer 3  -> Output Layer -> Transforms hidden layer output into a final prediction.

Then,
Fire or No Fire -> for inputs like x1 ,x2 ->
x1 + x2 > threshold -> Fire 
x1 + x2 < threshold -> No Fire
 
x1 + x2 - threshold > 0
x1 + x2 - threshold < 0

when (-threshold = bias)


Mainly there are 3 steps ->
1. Bias -> Bias is an extra parameter added to the weighted sum.
It helps the network make predictions even when all inputs are zero.

x1 + x2 + bias > 0  -> Fire
x1 + x2 + bias < 0  -> No Fire

2. Weights -> Weights are the learnable parameters that control the strength of the connection between neurons in one layer to the next.
Each input is multiplied by a weight before being passed forward.

y(output) = wo + x0 + w1 + x1 + w2 + x2 +.......+ wn + xn , ( x is the input)

3. Activation Function -> An activation function is applied to the output z of a neuron to introduce non-linearity.
(If not applied, Without it, a neural network would just be a big linear function — which can't model complex patterns)
Examples ->
1. ReLU    -> Most common for hidden layers                    - [0,∞)
2. Sigmoid -> Binary classification output                     - (0,1)
3. Tanh    -> Similar to sigmoid, but outputs between -1 and 1 - (-1,1)
4. Softmax -> Multi-class classification output                - (0,1)->(0,1), and all outputs sum to 1

Now Unnderstanding,
Forward Propogation -> Forward propagation is how a neural network takes input 
(like a picture or some numbers), processes it, and gives an output (like a prediction).

This process of detailed in the forward direction from input to output ->
Now At each layer, the network performs two main steps:
1. Linear transformation: Multiply inputs by weights and add bias.
2. Non-linear activation: Apply an activation function to the result.


Reducing Error in Neural Networks ->
1. we have 3 things - > input, weights , activation function ( only the weights help us decide because input cannot change , and activation function further depends on the weights and its outputs)
So we stick to changing the weights , between , input -> layer 1 , layer 1 -> layer 2 and so on.

Gradient descent ->

Error function -> (y-y^)^2/2
1. Should be continuous at each point 
2. Should be differentialable

and this has a convex property , helps us find the minimum
the error function also depends on the bias between the layers ,inout and output.

input -> layer -> output -> 
Error ( Wil,Bil,Wlo,Blo)->
W -> Weights, B -> Bias
Wil = Wil - Diff(E)/Diff(Wil) ... same for the other 3 as well.
Diff(E)/Diff(Wil) -> gives the slope of error with respect to weight b/w input and layer ... so on
Error 

Back Propogation in NN ->
1. Make a computational graph
2. Then create a chain rule to calculate the derivatives
Note: First step of back propogation is finding the error.


