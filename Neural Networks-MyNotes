3 types of layers ->
1. Layer 1  -> Input Layer  -> Takes input features
2. Layer 2  -> Hidden Layer -> Applies weights and biases, then passes through an activation function
3. Layer 3  -> Output Layer -> Transforms hidden layer output into a final prediction.

Then,
Fire or No Fire -> for inputs like x1 ,x2 ->
x1 + x2 > threshold -> Fire 
x1 + x2 < threshold -> No Fire
 
x1 + x2 - threshold > 0
x1 + x2 - threshold < 0

when (-threshold = bias)


Mainly there are 3 steps ->
1. Bias -> Bias is an extra parameter added to the weighted sum.
It helps the network make predictions even when all inputs are zero.

x1 + x2 + bias > 0  -> Fire
x1 + x2 + bias < 0  -> No Fire

2. Weights -> Weights are the learnable parameters that control the strength of the connection between neurons in one layer to the next.
Each input is multiplied by a weight before being passed forward.

y(output) = wo + x0 + w1 + x1 + w2 + x2 +.......+ wn + xn , ( x is the input)

3. Activation Function -> An activation function is applied to the output z of a neuron to introduce non-linearity.
(If not applied, Without it, a neural network would just be a big linear function — which can't model complex patterns)
Examples ->
1. ReLU    -> Most common for hidden layers                    - [0,∞)
2. Sigmoid -> Binary classification output                     - (0,1)
3. Tanh    -> Similar to sigmoid, but outputs between -1 and 1 - (-1,1)
4. Softmax -> Multi-class classification output                - (0,1)->(0,1), and all outputs sum to 1

Now Unnderstanding,
Forward Propogation -> Forward propagation is how a neural network takes input 
(like a picture or some numbers), processes it, and gives an output (like a prediction).

This process of detailed in the forward direction from input to output ->
Now At each layer, the network performs two main steps:
1. Linear transformation: Multiply inputs by weights and add bias.
2. Non-linear activation: Apply an activation function to the result.


Reducing Error in Neural Networks ->
1. we have 3 things - > input, weights , activation function ( only the weights help us decide because input cannot change , and activation function further depends on the weights and its outputs)
So we stick to changing the weights , between , input -> layer 1 , layer 1 -> layer 2 and so on.

Gradient descent ->

Error function -> (y-y^)^2/2
1. Should be continuous at each point 
2. Should be differentialable

and this has a convex property , helps us find the minimum
the error function also depends on the bias between the layers ,inout and output.

input -> layer -> output -> 
Error ( Wil,Bil,Wlo,Blo)->
W -> Weights, B -> Bias
Wil = Wil - Diff(E)/Diff(Wil) ... same for the other 3 as well.
Diff(E)/Diff(Wil) -> gives the slope of error with respect to weight b/w input and layer ... so on
Error 

Back Propogation in NN ->
1. Make a computational graph
2. Then create a chain rule to calculate the derivatives
Note: First step of back propogation is finding the error.

import torch
a = torch.ones(2, 2, requires_grad=True)
requires_grad=True -> makes sure the gradients are stored at this point.

b =a +5 
c = b.mean()
print(b)
print(c) 

output - > tensor([[6., 6.],
        [6., 6.]], grad_fn=<AddBackward0>)
tensor(6., grad_fn=<MeanBackward0>)

Explanation - >
When you perform an operation on a tensor with requires_grad=True, PyTorch builds a computational graph behind the scenes. Each operation creates a function object that knows how to compute the gradient of that operation.
These function objects are shown as:
<AddBackward0> for addition
<MeanBackward0> for mean
-> They are subclasses of PyTorch's internal Function class used during backward passes (i.e., .backward()).

then,
c.backward()        ->  # Triggers autograd to compute gradients
print(a.grad)       ->  # Shows: tensor([[0.25, 0.25],
                        #               [0.25, 0.25]])
output Explained ->
.backward() -> This is the correct way to trigger backpropagation.
It computes gradients of c with respect to all tensors that have requires_grad=True (like a)
It populates a.grad with those computed gradients

a.grad -> a.grad is where PyTorch stores the gradient of the final scalar output (c) w.r.t. tensor a
After you call c.backward(), you can access a.grad to get the actual gradient values

2. 
These optimizers are used to update the weights of neural networks during training using gradients computed by .backward().
from torch import optim  

use case - >

# SGD optimizer
sgd = optim.SGD(params, lr=0.01)

# Adam optimizer
adam = optim.Adam(params, lr=0.001)

3. Autograd module in pytorch helps us define computational graphs, as we proceed.
Coding the whole neural network from strach ->
import torch

# Input tensor
X = torch.Tensor([
    [1, 0, 1, 0],
    [1, 0, 1, 1],
    [0, 1, 0, 1]
])

# Output tensor
y = torch.Tensor([
    [1],
    [1],
    [0]
])

print("Input X:\n", X, '\n')
print("Output y:\n", y)

# Sigmoid activation function
def sigmoid(x):
    return 1 / (1 + torch.exp(-x))

# Derivative of sigmoid function
def derivatives_sigmoid(x):
    return sigmoid(x) * (1 - sigmoid(x))

Step 2 ->
# Variable initialization
epoch = 7000                  # Number of training iterations (epochs)
lr = 0.1                     # Learning rate - controls how big a step is taken during optimization

inputlayer_neurons = X.shape[1]      # Number of features in the input data (number of columns in X)
hiddenlayer_neurons = 3               # Number of neurons in the hidden layer (you choose this)
output_neurons = 1                    # Number of neurons in the output layer (for binary output, 1 neuron)

# Weight and bias initialization
wh = torch.randn(inputlayer_neurons, hiddenlayer_neurons).type(torch.FloatTensor)  # Weights between input and hidden layer
bh = torch.randn(1, hiddenlayer_neurons).type(torch.FloatTensor)                   # Bias for hidden layer
wout = torch.randn(hiddenlayer_neurons, output_neurons).type(torch.FloatTensor)    # Weights between hidden and output layer
bout = torch.randn(1, output_neurons).type(torch.FloatTensor)                      # Bias for output layer

Explanation - > 
Purpose of .type(torch.FloatTensor)
It converts the tensor to a 32-bit float tensor explicitly.

wh = torch.randn(inputlayer_neurons, hiddenlayer_neurons) ->
inputlayer_neurons = number of features in your input data.
hiddenlayer_neurons = number of neurons in the hidden layer.

bh = torch.randn(1, hiddenlayer_neurons) ->
The shape is (1, hiddenlayer_neurons), meaning:
hiddenlayer_neurons = number of neurons in the hidden layer.
So if you have, say, 3 hidden neurons, bh holds 3 bias values — one for each neuron.
->
Each neuron sums all its weighted inputs plus a single bias term.
The bias acts like an extra input always set to 1, shifting the neuron’s activation.

similarly for wout, bout 

continuation -> 
for i in range(epoch):
    # Forward Propagation
    hidden_layer_input1 = torch.mm(X, wh)                            -> mm:matrix multiplication
    hidden_layer_input = hidden_layer_input1 + bh
    hidden_layer_activations = sigmoid(hidden_layer_input)
    
    output_layer_input1 = torch.mm(hidden_layer_activations, wout)
    output_layer_input = output_layer_input1 + bout
    output = sigmoid(output_layer_input)
    
    # Backpropagation
    E = y - output
    slope_output_layer = derivatives_sigmoid(output)
    slope_hidden_layer = derivatives_sigmoid(hidden_layer_activations)
    
    d_output = E * slope_output_layer
    Error_at_hidden_layer = torch.mm(d_output, wout.t())
    d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer
    
    wout += torch.mm(hidden_layer_activations.t(), d_output) * r
    bout += d_output.sum() * r
    wh += torch.mm(X.t(), d_hiddenlayer) * r
    bh += d_hiddenlayer.sum() * r

| Code                                                             | Math Notation           | Explanation                                              |
| ---------------------------------------------------------------- | ----------------------- | -------------------------------------------------------- |
| `hidden_layer_input1 = torch.mm(X, wh)`                          | $Z_h = X W_h$           | Linear combination of inputs and weights to hidden layer |
| `hidden_layer_input = hidden_layer_input1 + bh`                  | $Z_h = Z_h + b_h$       | Add bias to hidden layer input                           |
| `hidden_layer_activations = sigmoid(hidden_layer_input)`         | $A_h = \sigma(Z_h)$     | Apply sigmoid activation function                        |
| `output_layer_input1 = torch.mm(hidden_layer_activations, wout)` | $Z_o = A_h W_o$         | Linear combination at output layer                       |
| `output_layer_input = output_layer_input1 + bout`                | $Z_o = Z_o + b_o$       | Add bias to output layer input                           |
| `output = sigmoid(output_layer_input)`                           | $\hat{y} = \sigma(Z_o)$ | Output layer activation (prediction)                     |

| Code             | Math Notation     | Explanation                                    |
| ---------------- | ----------------- | ---------------------------------------------- |
| `E = y - output` | $E = y - \hat{y}$ | Difference between true labels and predictions |

| Code                                                                 | Math Notation                       | Explanation                               |
| -------------------------------------------------------------------- | ----------------------------------- | ----------------------------------------- |
| `slope_output_layer = derivatives_sigmoid(output)`                   | $\sigma'(Z_o)$                      | Derivative of sigmoid at output layer     |
| `slope_hidden_layer = derivatives_sigmoid(hidden_layer_activations)` | $\sigma'(Z_h)$                      | Derivative of sigmoid at hidden layer     |
| `d_output = E * slope_output_layer`                                  | $\delta_o = E \circ \sigma'(Z_o)$   | Output layer delta (element-wise product) |
| `Error_at_hidden_layer = torch.mm(d_output, wout.t())`               | $E_h = \delta_o W_o^T$              | Error propagated back to hidden layer     |
| `d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer`         | $\delta_h = E_h \circ \sigma'(Z_h)$ | Hidden layer delta                        |

| Symbol    | Meaning                                        | Shape / Description                                              |
| --------- | ---------------------------------------------- | ---------------------------------------------------------------- |
| $X$       | Input data matrix                              | $(m \times n)$ — $m$: number of samples, $n$: number of features |
| $y$       | True labels                                    | $(m \times p)$ — $p$: number of output neurons                   |
| $W_h$     | Weights between input and hidden layer         | $(n \times h)$ — $h$: number of hidden neurons                   |
| $b_h$     | Bias vector for hidden layer                   | $(1 \times h)$                                                   |
| $W_o$     | Weights between hidden and output layer        | $(h \times p)$                                                   |
| $b_o$     | Bias vector for output layer                   | $(1 \times p)$                                                   |
| $\sigma$  | Sigmoid activation function                    | Applied element-wise                                             |
| $\sigma'$ | Derivative of sigmoid function                 | $\sigma'(x) = \sigma(x) \times (1 - \sigma(x))$                  |
| $\circ$   | Element-wise multiplication (Hadamard product) |                                                                  |
| $r$       | Learning rate                                  | Scalar controlling step size in gradient descent                 |
| $m$       | Number of samples                              | Batch size or dataset size                                       |
| $n$       | Number of input features                       | Dimensionality of input data                                     |
| $h$       | Number of hidden neurons                       | Size of hidden layer                                             |
| $p$       | Number of output neurons                       | Output dimensionality                                            |


continuation ->
print("Actual:\n", y, "\n")
print("Predicted:\n", output, "\n")

print("So, the target labels are 1, 1, and 0.")
print("The model's predicted values are approximately 0.98, 0.97, and 0.03 respectively.")
print("Not bad — the model predictions are very close to the actual values!")

output - >
# y = torch.Tensor([[1], [1], [0]])
# output = torch.Tensor([[0.9834], [0.9780], [0.0330]])


