Tensor flow playground - https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.53335&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false
In training a neural network, you notice that the loss does not decrease in the few starting epochs ->
| Cause                             | Can prevent early loss drop? |
| --------------------------------- | ---------------------------- |
| I. Learning rate is too low       | ✅ Yes                        |
| II. Regularization parameter high | ✅ Yes                        |
| III. Stuck in local minimum       | ✅ Yes                        |

Note : In backwards propogation weights and biases can be updated!

To manually check the good weights and bias for less error ->
we plot the cost function/loss function v/s weight /bias graph to check the minima.

Important factors to reach the local minima ->
1. In which direction should I move ?
2. How much should I move ?

Here, gradient descent is helpful -> (cost/loss in y-axis , and weight/bias in x-axis)
w = w - learning rate * diff(E)/diff(w) -> diff of Error/weight
diff(E)/diff(w) -> +ve or -ve tell us which direction to move in.
                -> value of it tells us how much we should move in that direction.

Factors to stop the gradient descent ->
1. If the Error is not updating.
2. Number of iterations/epochs are reached.

Note : For binary classification problem a single layer in the output is enough for the problem.

Forward propogation ->
Y =W(t) * X -> X is input, W(t) is weights, also the bias is added here.
Example -> (4 input, 5 hidden ,1 output) -> (4 classes with 6 observations each as input) (output is 1 class with 6 observations)
X     -> 4 * 6
W(ih) -> 4 * 5  
B(ih) -> 5 * 1 (5,) -> 2d numpy array
-> Z(1) =W(ih)T * X + B(ih)  -> 5 * 6 
-> h(1) = sigmoid(Z(1))      -> 5 * 6 

Z(2) = W(ho)T * h(1) + B(ho) -> 1 * 6 
O = sigmoid(Z(2))  -> 1 * 6

Backward propogation -> desktop-> analytics vidya -> saved in neural networds -> pdf file 

Numpy steps to build the model ->
1. Loading the dataset (Input and Output)
2. Architecture of the model (# input, hidden and output neurons)
3. Initializing the weights for all the layers
4. Implementing forward propagation
5. Implementing backward propagation
6. Train the model for n epochs

Analytics Vidya/Neural Network/Neural_Network_from_scratch_using_NumPy.ipynb
CODE -> (Imp terms)
np.random.uniform(low=0.0, high=1.0, size=(...)) -> 
generates random numbers from a uniform distribution, which means:
Each number in the interval [low, high) has an equal probability of being selected.

np.random.uniform(size=(2, 3)) -> ( 2 rows , 3 columns)
array([[0.41, 0.78, 0.23],
       [0.55, 0.01, 0.97]])

In NumPy, .dot() performs a: Dot product — a form of matrix multiplication (or scalar product for 1D arrays).
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

np.dot(a,b) -> x = 32  # scalar (single number)
a * b       -> x = np.array([4, 10, 18])  # same shape as a and b

Note -> Always know when to use transpose, this can be made sure by looking at the shapes of the matrices.
error_wrt_wih.shape -> (4,3) -> w_ih.shape
error_wrt_who.shape -> (3,1) -> w_ho.shape
Imp : 
# rate of change of error w.r.t weight between hidden and output layer
error_wrt_who = np.dot(Z2_wrt_who,(error_wrt_output*output_wrt_Z2).T)

# rate of change of error w.r.t weights between input and hidden layer
error_wrt_wih = np.dot(Z1_wrt_wih,(h1_wrt_Z1*np.dot(Z2_wrt_h1,(output_wrt_Z2*error_wrt_output))).T)

# visualizing the error after each epoch
plt.plot(np.arange(1,epochs+1), np.array(error_epoch))

np.arange(1, epochs + 1) ->
A sequence from 1 to epochs inclusive ( 1and epoch are included that is why we use till 1+ epoch)
Because np.arange() excludes the stop value, you need epochs + 1 to include the final epoch

 error_epoch.append(np.average(error)) -> np.average() is required because :
y       = [1, 0, 1, 0, 1, 0]
output  = [0.9, 0.1, 0.8, 0.3, 0.7, 0.2]

error = (y - output)² / 2
      = [0.005, 0.005, 0.02, 0.045, 0.045, 0.02]  # shape: (1, 6) -> here the avarage of these error list is taken / per epoch 

Activation Function -> if not used then it is set to linear function, cannot be used for any other complex patterns.
1. Linear -> used mostly at the ouput layer for a regression problem. 
input  -> - infi to + infi
output -> - infi to + infi

2. Sigmoid -> used mostly at the ouput layer for a probability in classification problem. 
input  -> - infi to + infi
output ->  0 to 1

3. Tanh -> faster than sigmoid
input  -> - infi to + infi
output ->  -1 to 1

4. ReLu -> Not differentiable at x = 0 
input  -> - infi to + infi
output ->  0 to + infi
Derivative -> 0.01 (x <=0) 1 (x>0) not defined (x = 0)

5. ReLu leaky -> used mostly at the ouput layer for a probability in classification problem. 
input  -> - infi to + infi
output ->  0.01x(when -ve) to x
Derivative -> 0.01 (x <=0) 1 (x>0)

In case we have more than 1 output ( i.e we have 3 classes output neurons) 
to get a realtion between them we use softmax activation function so that they are relative to each other.(this is help to better predict the neuron)
6. Softmax -> used mostly at the ouput layer for a probability in multi class classification problem. 
Formula =  e^(Zi)/ summation of e^(Zi) -> (and the summation of zi =1 )
Sum of probabilities of a softmax function is 1.
input  -> - infi to + infi
| Property                 | Explanation                                        |
| ------------------------ | -------------------------------------------------- |
| Output range             | Each value is in **(0, 1)** (exclusive)            |
| All outputs add to 1     | (\sum_{i=1}^n \text{softmax}(z_i) = 1)             |
| Represents probabilities | Can be interpreted as **confidence** in each class |
| Most confident class     | Has the **highest softmax score**                  |

Note : Hidden layers ReLu > tanh ( i.e ReLu is more preffered)

1. Stochastic -> 
1. Only one onservation at one time is taken, and so on for the rest
2.add a shuffle parameter before selection of the records
3. improved model performace beacause of the frequent updation of weights and biases
4. updating the weights based on just one sample can result in fluctuations in loss value.

2.Batch gradient descent ->
all observations at once.
we use all the records in the dataset for forward propogation, use the aggregated loss value to update the weights.
more stable and less frequent updates 
i.e 50 samples, 10 epochs we will be updating the weights just 10 times.
Advantages ->
 computational efficiency, Stable Error gradient, Faster convergence
Dis ->
would need several epochs for training, requires the full data memory everytime, less scalability.

3. Mini-Batch gradient descent ->
Divides the training dataset into subsets
Then, forward propogation on the subset, loss function, updates the weights of the model based on the subset.
i.e 50 samples 10 batches -> 5 subsets, 5 epochs = 25 times
Note : Mini batch size of 2^n is preffered

Refer table for better understanding ->
( inputs size : 100, epochs : 100, batch size : 10(mini batch)) 
| Type           | Data Used per Update  | Updates per Epoch | Pros                              | Cons                            | Updates 
| -------------- | --------------------- | ----------------- | --------------------------------- | ------------------------------- |
| **SGD**        | 1 sample              | N (1 per sample)  | Fast updates, low memory          | Noisy updates, less stable      |10000
| **Batch**      | All data              | 1                 | Stable, efficient with small data | Needs full memory, slow updates |100
| **Mini-Batch** | Small batch (e.g. 32) | N / batch size    | Balanced, fast & stable           | Needs batch size tuning         |1000


1. To avoid getting stuck at local minima we use gradient descent with momentum. 
(i.e example of a ball, even though the ball reaches local minima it still has some past speed to it which helps it to bounce back, this is used in the momentum concept)
Implementation in the above code is - >
 # weighted gradient
    v_ho = beta * v_ho + (1-beta)*error_wrt_who
    w_ho = w_ho - lr * v_ho
    v_ih = beta * v_ih + (1-beta)*error_wrt_wih
    w_ih = w_ih - lr * v_ih

where as the regular without momemtum looks like ->
    # updating the weights between hidden and output layer
    w_ho = w_ho - lr * error_wrt_who
    # updating the weights between input and hidden layer
    w_ih = w_ih - lr * error_wrt_wih



