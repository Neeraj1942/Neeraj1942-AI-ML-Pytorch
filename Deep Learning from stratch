Tensor flow playground - https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.53335&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false
In training a neural network, you notice that the loss does not decrease in the few starting epochs ->
| Cause                             | Can prevent early loss drop? |
| --------------------------------- | ---------------------------- |
| I. Learning rate is too low       | ✅ Yes                        |
| II. Regularization parameter high | ✅ Yes                        |
| III. Stuck in local minimum       | ✅ Yes                        |

Note : In backwards propogation weights and biases can be updated!

To manually check the good weights and bias for less error ->
we plot the cost function/loss function v/s weight /bias graph to check the minima.

Important factors to reach the local minima ->
1. In which direction should I move ?
2. How much should I move ?

Here, gradient descent is helpful -> (cost/loss in y-axis , and weight/bias in x-axis)
w = w - learning rate * diff(E)/diff(w) -> diff of Error/weight
diff(E)/diff(w) -> +ve or -ve tell us which direction to move in.
                -> value of it tells us how much we should move in that direction.

Factors to stop the gradient descent ->
1. If the Error is not updating.
2. Number of iterations/epochs are reached.

Note : For binary classification problem a single layer in the output is enough for the problem.

Forward propogation ->
Y =W(t) * X -> X is input, W(t) is weights, also the bias is added here.
Example -> (4 input, 5 hidden ,1 output) -> (4 classes with 6 observations each as input) (output is 1 class with 6 observations)
X     -> 4 * 6
W(ih) -> 4 * 5  
B(ih) -> 5 * 1 (5,) -> 2d numpy array
-> Z(1) =W(ih)T * X + B(ih)  -> 5 * 6 
-> h(1) = sigmoid(Z(1))      -> 5 * 6 

Z(2) = W(ho)T * h(1) + B(ho) -> 1 * 6 
O = sigmoid(Z(2))  -> 1 * 6

Backward propogation -> desktop-> analytics vidya -> saved in neural networds -> pdf file 

Numpy steps to build the model ->
1. Loading the dataset (Input and Output)
2. Architecture of the model (# input, hidden and output neurons)
3. Initializing the weights for all the layers
4. Implementing forward propagation
5. Implementing backward propagation
6. Train the model for n epochs

Analytics Vidya/Neural Network/Neural_Network_from_scratch_using_NumPy.ipynb
CODE -> (Imp terms)
np.random.uniform(low=0.0, high=1.0, size=(...)) -> 
generates random numbers from a uniform distribution, which means:
Each number in the interval [low, high) has an equal probability of being selected.

np.random.uniform(size=(2, 3)) -> ( 2 rows , 3 columns)
array([[0.41, 0.78, 0.23],
       [0.55, 0.01, 0.97]])

In NumPy, .dot() performs a: Dot product — a form of matrix multiplication (or scalar product for 1D arrays).
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

np.dot(a,b) -> x = 32  # scalar (single number)
a * b       -> x = np.array([4, 10, 18])  # same shape as a and b

Note -> Always know when to use transpose, this can be made sure by looking at the shapes of the matrices.
error_wrt_wih.shape -> (4,3) -> w_ih.shape
error_wrt_who.shape -> (3,1) -> w_ho.shape
Imp : 
# rate of change of error w.r.t weight between hidden and output layer
error_wrt_who = np.dot(Z2_wrt_who,(error_wrt_output*output_wrt_Z2).T)

# rate of change of error w.r.t weights between input and hidden layer
error_wrt_wih = np.dot(Z1_wrt_wih,(h1_wrt_Z1*np.dot(Z2_wrt_h1,(output_wrt_Z2*error_wrt_output))).T)

# visualizing the error after each epoch
plt.plot(np.arange(1,epochs+1), np.array(error_epoch))

np.arange(1, epochs + 1) ->
A sequence from 1 to epochs inclusive ( 1and epoch are included that is why we use till 1+ epoch)
Because np.arange() excludes the stop value, you need epochs + 1 to include the final epoch

 error_epoch.append(np.average(error)) -> np.average() is required because :
y       = [1, 0, 1, 0, 1, 0]
output  = [0.9, 0.1, 0.8, 0.3, 0.7, 0.2]

error = (y - output)² / 2
      = [0.005, 0.005, 0.02, 0.045, 0.045, 0.02]  # shape: (1, 6) -> here the avarage of these error list is taken / per epoch 

Activation Function -> if not used then it is set to linear function, cannot be used for any other complex patterns.
1. Linear -> used mostly at the ouput layer for a regression problem. 
input  -> - infi to + infi
output -> - infi to + infi

2. Sigmoid -> used mostly at the ouput layer for a probability in classification problem. 
input  -> - infi to + infi
output ->  0 to 1

3. Tanh -> faster than sigmoid
input  -> - infi to + infi
output ->  -1 to 1

4. ReLu -> Not differentiable at x = 0 
input  -> - infi to + infi
output ->  0 to + infi
Derivative -> 0.01 (x <=0) 1 (x>0) not defined (x = 0)

5. ReLu leaky -> used mostly at the ouput layer for a probability in classification problem. 
input  -> - infi to + infi
output ->  0.01x(when -ve) to x
Derivative -> 0.01 (x <=0) 1 (x>0)

In case we have more than 1 output ( i.e we have 3 classes output neurons) 
to get a realtion between them we use softmax activation function so that they are relative to each other.(this is help to better predict the neuron)
6. Softmax -> used mostly at the ouput layer for a probability in multi class classification problem. 
Formula =  e^(Zi)/ summation of e^(Zi) -> (and the summation of zi =1 )
Sum of probabilities of a softmax function is 1.
input  -> - infi to + infi
| Property                 | Explanation                                        |
| ------------------------ | -------------------------------------------------- |
| Output range             | Each value is in **(0, 1)** (exclusive)            |
| All outputs add to 1     | (\sum_{i=1}^n \text{softmax}(z_i) = 1)             |
| Represents probabilities | Can be interpreted as **confidence** in each class |
| Most confident class     | Has the **highest softmax score**                  |

Note : Hidden layers ReLu > tanh ( i.e ReLu is more preffered)


