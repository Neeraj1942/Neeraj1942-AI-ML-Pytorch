RNN ->
Sequential Modelling ->
Can work with time series data , audio data, video frames, text etc.
Sequential prediction is import to predict the next steos in the model
So remeber the past data , we use a added weight to hidden layers and connect these hidde layers.
Task : classify sentiment analysis of the text as positive or negative -> many to one model


        timestep 1                timestep 2                timestep 3
    ─────────────────        ─────────────────        ─────────────────
          x1                        x2                        x3
          │                         │                         │
          │  U                      │  U                      │  U
          ▼                         ▼                         ▼
        ┌─────┐       W           ┌─────┐        W          ┌─────┐
H0 ---> │ H1  │ --------------->  │ H2  │  --------------->  │ H3  │
(initial│     │                   │     │                    │     │
 state) └─────┘                   └─────┘                    └─────┘
                                                              │
                                                              │  V
                                                              ▼
                                                            ŷ (output)


Forward Propogation ->
Many to one example : 
H1 = g(W * H0 + U * X1)
H2 = g(W * H1 + U * X2)
H3 = g(W * H2 + U * X3)
Y = k(V * H3)
U,V,W -> weights, can be considered as gradients
| Symbol  | Meaning                                                           |
| ------- | ----------------------------------------------------------------- |
| ( x_t ) | Input vector at time step ( t ) (e.g., word embedding for a word) |
| ( h_t ) | Hidden state (like memory) at time step ( t )                     |
| ( W )   | Hidden-to-hidden weights (same at every step)                     |
| ( U )   | Input-to-hidden weights                                           |
| ( V )   | Hidden-to-output weights                                          |
| ( g )   | Activation function (usually tanh or ReLU) non linear             |
| ( k )   | Activation function (usually sigmoid,linear, softmax ) linear     |                                            

Backward Propogation ->
dl/dv = dl/dy * dy/dv 
now if the loss function -> L = 1/2(y-y^)^2
dl/dy = (y-y^) * dy/dv ; if considered a linear activation function -> k =1 
then, y = VH3 -> dy/dv = h3 ; using this above equation 
dl/dv = (y-y^) * H3

dl/dw = dl/dy * dy/dH * dH/dW

dl/dU = dl/dy * dy/dH * dH/dU

Weight update equation : 
v = v -alpha * dl/dv
w = w -alpha * dl/dw
u = u -alpha * dl/du


